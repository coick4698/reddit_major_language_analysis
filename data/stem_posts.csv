id,title,selftext,created_utc,subreddit,group
1ja3y0y,How does CS research work anyway? A.k.a. How to get into a CS research group?,"One question that comes up fairly frequently both here and on other subreddits is about getting into CS research. So I thought I would break down how research group (or labs) are run. This is based on my experience in 14 years of academic research, and 3 years of industry research. This means that yes, you might find that at your school, region, country, that things work differently. I'm not pretending I know how everything works everywhere.

Let's start with what research gets done:

**The professor's personal research program.**

Professors don't often do research directly (they're too busy), but some do, especially if they're starting off and don't have any graduate students. You have to publish to get funding to get students. For established professors, this line of work is typically done by research assistants.

Believe it or not, this is actually a really good opportunity to get into a research group at all levels by being hired as an RA. The work isn't glamourous. Often it will be things like building a website to support the research, or a data pipeline, but is is research experience.

**Postdocs**.

A postdoc is somebody that has completed their PhD and is now doing research work within a lab. The postdoc work is usually at least somewhat related to the professor's work, but it can be pretty diverse. Postdocs are paid (poorly). They tend to cry a lot, and question why they did a PhD. :)

If a professor has a postdoc, then try to get to know the postdoc. Some postdocs are jerks because they're have a doctorate, but if you find a nice one, then this can be a great opportunity. Postdocs often like to supervise students because it gives them supervisory experience that can help them land a faculty position. Professor don't normally care that much if a student is helping a postdoc as long as they don't have to pay them. Working conditions will really vary. Some postdocs do \*not\* know how to run a program with other people.

**Graduate Students**.

PhD students are a lot like postdocs, except they're usually working on one of the professor's research programs, unless they have their own funding. PhD students are a lot like postdocs in that they often don't mind supervising students because they get supervisory experience. They often know even less about running a research program so expect some frustration. Also, their thesis is on the line so if you screw up then they're going to be \*very\* upset. So expect to be micromanaged, and try to understand their perspective.

Master's students also are working on one of the professor's research programs. For my master's my supervisor literally said to me ""Here are 5 topics. Pick one."" They don't normally supervise other students. It might happen with a particularly keen student, but generally there's little point in trying to contact them to help you get into the research group.

**Undergraduate Students**.

Undergraduate students might be working as an RA as mentioned above. Undergraduate students also do a undergraduate thesis. Professors like to steer students towards doing something that helps their research program, but sometimes they cannot so undergraduate research can be \*extremely\* varied inside a research group. Although it will often have some kind of connective thread to the professor. Undergraduate students almost never supervise other students unless they have some kind of prior experience. Like a master's student, an undergraduate student really cannot help you get into a research group that much.

**How to get into a research group**

There are four main ways:

1. Go to graduate school. Graduates get selected to work in a research group. It is part of going to graduate school (with some exceptions). You might not get into the research group you want. Student selection works different any many school. At some schools, you have to have a supervisor before applying. At others students are placed in a pool and selected by professors. At other places you have lab rotations before settling into one lab. It varies a lot.
2. Get hired as an RA. The work is rarely glamourous but it is research experience. Plus you get paid! :) These positions tend to be pretty competitive since a lot of people want them.
3. Get to know lab members, especially postdocs and PhD students. These people have the best chance of putting in a good word for you.
4. Cold emails. These rarely work but they're the only other option.

**What makes for a good email**

1. Not AI generated. Professors see enough AI generated garbage that it is a major turn off.
2. Make it personal. You need to tie your skills and experience to the work to be done.
3. Do not use a form letter. It is obvious no matter how much you think it isn't.
4. Keep it concise but detailed. Professor don't have time to read a long email about your grand scheme.
5. Avoid proposing research. Professors already have plenty of research programs and ideas. They're very unlikely to want to work on yours.
6. Propose research (but only if you're applying to do a thesis or graduate program). In this case, you need to show that you have some rudimentary idea of how you can extend the professor's research program (for graduate work) or some idea at all for an undergraduate thesis.

It is rather late here, so I will not reply to questions right away, but if anyone has any questions, the ask away and I'll get to it in the morning.",1741839950.0,computerscience,STEM
1j64cf5,Books and Resources,"Hi, r/computerscience. 

We've updated our books and resources list with the latest recommendations from the past four months. Before asking for resources on a specific topic, please check this list to see if this has already been solved. This helps us keep things organized and avoid other members of our community seeing the same post twice a week.

If you have suggestions, feel free to add them. We do not advertise and we discourage this, so please avoid attaching referral links to courses/books as this is something we will ban. The entire purpose of this is to help those that are curious or need a little guidance, not to materialize.

If your topic isn’t covered in the current list, don’t hesitate to ask below.

NOTE: This is a section to ask what is stated in the title (i.e., books and resources), not to ask for career advice (rule 3) or help with your homework (rule 8).

// ###

Computer architecture: [https://www.reddit.com/r/computerscience/comments/1itqnyv/which\_book\_is\_good\_for\_computer\_architetcure/](https://www.reddit.com/r/computerscience/comments/1itqnyv/which_book_is_good_for_computer_architetcure/)

Computer networks: [https://www.reddit.com/r/computerscience/comments/1iijm8a/computer\_netwroks\_a\_top\_down\_approach/](https://www.reddit.com/r/computerscience/comments/1iijm8a/computer_netwroks_a_top_down_approach/)

Discrete math: [https://www.reddit.com/r/computerscience/comments/1hcz7jc/what\_are\_the\_best\_books\_on\_discrete\_mathematics/](https://www.reddit.com/r/computerscience/comments/1hcz7jc/what_are_the_best_books_on_discrete_mathematics/)

Interpreters and compilers: [https://www.reddit.com/r/computerscience/comments/1h3ju2h/looking\_for\_bookscourses\_on\_interpreterscompilers/](https://www.reddit.com/r/computerscience/comments/1h3ju2h/looking_for_bookscourses_on_interpreterscompilers/)

Hardware: [https://www.reddit.com/r/computerscience/comments/1i711c8/best\_books\_for\_learning\_hardware\_of\_computers/](https://www.reddit.com/r/computerscience/comments/1i711c8/best_books_for_learning_hardware_of_computers/)

History of software engineering: [https://www.reddit.com/r/computerscience/comments/1grrjud/what\_software\_engineering\_history\_book\_do\_you\_like/](https://www.reddit.com/r/computerscience/comments/1grrjud/what_software_engineering_history_book_do_you_like/)

Donald Knuth books: [https://www.reddit.com/r/computerscience/comments/1ixmn3m/donald\_knuth\_and\_his\_books/](https://www.reddit.com/r/computerscience/comments/1ixmn3m/donald_knuth_and_his_books/)

Bjarne Stroustrup C++: [https://www.reddit.com/r/computerscience/comments/1iy6lot/is\_there\_a\_shorter\_bjarne\_stroustrup\_book\_on\_c/](https://www.reddit.com/r/computerscience/comments/1iy6lot/is_there_a_shorter_bjarne_stroustrup_book_on_c/)

// ###

What's on Your Bookshelves? [https://www.reddit.com/r/computerscience/comments/1hkycga/whats\_on\_your\_bookshelves\_recommendations\_for/](https://www.reddit.com/r/computerscience/comments/1hkycga/whats_on_your_bookshelves_recommendations_for/)

\[Easy reads\] Reading while munching: [https://www.reddit.com/r/computerscience/comments/1h3ouy3/resources\_for\_learning\_some\_new\_things/](https://www.reddit.com/r/computerscience/comments/1h3ouy3/resources_for_learning_some_new_things/)

// ###

Getting into CS Research: [https://www.reddit.com/r/computerscience/comments/1ip1w63/getting\_into\_cs\_research/](https://www.reddit.com/r/computerscience/comments/1ip1w63/getting_into_cs_research/)

Hot topics in CS: [https://www.reddit.com/r/computerscience/comments/1h4e31y/what\_are\_currently\_the\_hot\_topics\_in\_computer/](https://www.reddit.com/r/computerscience/comments/1h4e31y/what_are_currently_the_hot_topics_in_computer/)

// ###

These are some other interesting questions looking for resources that did not get a lot of input, but I consider brilliant:

Learning complex software for embedded systems: [https://www.reddit.com/r/computerscience/comments/1iqikdh/learning\_complex\_software\_for\_embedded\_systems/](https://www.reddit.com/r/computerscience/comments/1iqikdh/learning_complex_software_for_embedded_systems/)

Low level programming and IC design: [https://www.reddit.com/r/computerscience/comments/1ghwlgr/low\_level\_programming\_and\_ic\_design\_resources/](https://www.reddit.com/r/computerscience/comments/1ghwlgr/low_level_programming_and_ic_design_resources/)

OS and IOT books: [https://www.reddit.com/r/computerscience/comments/1h4vvra/looking\_for\_os\_and\_iot\_books/](https://www.reddit.com/r/computerscience/comments/1h4vvra/looking_for_os_and_iot_books/)

System design: [https://www.reddit.com/r/computerscience/comments/1gh8ibp/practice\_with\_system\_design/](https://www.reddit.com/r/computerscience/comments/1gh8ibp/practice_with_system_design/)

Satellite Communication: [https://www.reddit.com/r/computerscience/comments/1h874ik/seeking\_recommendations\_for\_books\_on\_using\_code/](https://www.reddit.com/r/computerscience/comments/1h874ik/seeking_recommendations_for_books_on_using_code/)

// ###

About “staying updated” in the field: [https://www.reddit.com/r/computerscience/comments/1hga9tu/how\_do\_you\_stay\_updated\_with\_the\_tech\_world/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/computerscience/comments/1hga9tu/how_do_you_stay_updated_with_the_tech_world/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)

If you need a gift for someone special in computer science, or would like to add suggestions: [https://www.reddit.com/r/computerscience/comments/1igw21l/valentines\_day\_gift\_ideas/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/computerscience/comments/1igw21l/valentines_day_gift_ideas/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)",1741392583.0,computerscience,STEM
1m2ydqg,Books Every Computer Science Student Should Read,,1752833216.0,computerscience,STEM
1m3s8oc,Just noticed this typo,Hard to believe they got Brian Kernighan's name wrong on his own book. I've had it for years and somehow never noticed. Is it on anyone else's?,1752918135.0,computerscience,STEM
1m3sxff,Selling projects requested.,"Dm me for any projects that you’re looking to have made. (BOTS, SERVICES, SITES WHATEVERYOURE LOOKING FOR.) (Must be Python/c++) CHEAP RATES.!! (Looking to make an extra couple dollars as I’m unemployed right now. Therefore I can provide computer programming services.) Would appreciate anybody who DM’s me. ",1752920840.0,computerscience,STEM
1m3sqk1,Sourceduty Knowledge,[https://sourceduty.com/](https://sourceduty.com/),1752920085.0,computerscience,STEM
1m2xw48,Undone CS 2026 : 2nd conference on Undone Science in Computer Science,,1752831444.0,computerscience,STEM
1m35k2h,How to get excited/love CS?,"Due to unforeseen circumstances against my will ( health and financial issues), I couldn’t continue in the medical field and had to switch fields after trying for 3 years in med, and my only and best option is CS, which is what Im joining 

He.lp me get exc.ited for CS (if fun, curiosity and creativity is in ANY subj I can Love it)",1752852928.0,computerscience,STEM
1m1swut,Scalability is not performance,,1752710925.0,computerscience,STEM
1m1mchl,Question regarding the L4 section in the OSI model.,"Howdy! I'm trying to get into networking and I'm enjoying it so far, but I'm having a hard time understanding the OSI model.

My question is- Does the L4 sector split the data into segments, adds an L4 header to each of em and sends it down to the lower levels OR does it put the L4 header on the whole block of data and the splitting happens in some other weird way :#.  
  
 I know the question sounds stupid, but I'm getting mixed answers lmao.",1752694728.0,computerscience,STEM
1m0z0uc,Can we measure efficiency brought by abstraction?,"I was wondering if abstraction is made purely for  humans to organize and comprehend things better. 

If there is an intelligence that has no human limitations in terms of computation and memory, will it ever use abstraction to pursue efficiency?

Sorry, I’m having trouble wording this out, but it came from the thought that abstraction ends up causing space inefficiency (probably why C or C++ is used). Then the reason why we use it seems to be for humans to organize and comprehend large amounts of code and data better, but if our brain does not have this limitation, will abstraction be used at all? If it’s used because it can guide to where the information is better, can we measure the efficiency brought? Abstraction kind of feels like algorithms in this case (brute force vs algorithmic trials), and I was wondering if there’s a way to measure this.

I wonder if there’s a related theory to this or any studies out there that deals something similar to this. Thanks for reading guys appreciate any insights. ",1752627069.0,computerscience,STEM
1m0f83g,Can that thing be a working CPU for my computer?,"So basically it's for my redstone computer in Minecraft but it doesn't matter here. On the top you can see 4 cores, each one with their control unit (CU) and personal registers as well as ALU. The clock generates signals with a delay and it's basically the same as CPU's work with ticks to perform an action. Then you have the instruction register (IR) which stores the current instruction, and the instruction decoder. The circles are the wires to communicate with my GPU and SSD. 



If it's missing some information and you have questions, ask!!",1752578752.0,computerscience,STEM
1m0t1og,I’m worried that I’m cheating myself when using libraries,,1752611778.0,computerscience,STEM
1lyd3gu,"Realistically speaking, if you were to pursue a PHD, what topics can you even research anymore?","Let's say you want to become an uni professor and you require a PHD, what subjects can you talk about and research that hasn't already been discussed? Can you even come up with a brand new topic anymore? Am I missing something?  
  
You're not into Artificial Intelligence, Machine Learning, Embedded, whatever, you're the classic Frontend/Backend/DevOps/QA/Mobile/etc engineer. What can you even tackle worthy of a thesis?",1752359768.0,computerscience,STEM
1lyjxrq,How can Computational Neuroscience explain the Origin of First-Person Subjectivity: How Do I Feel Like “Me”?,"There exists a compelling tension between how we experience subjectivity and how we understand the brain scientifically. While cognitive neuroscience studies the brain as a physical organ—complex networks of neurons firing unconsciously—our immediate experience treats subjectivity as a vivid, unified, conscious presence. Although one might say the brain and the self are aspects of the same system described at different levels, this does not explain why Subjectivity feels the way it feels.

The central dilemma is paradoxical by design:

\>There is no one who has experience—only the experience of being someone.

Cognitive Scientist Thomas Metzinger says This is not wordplay. We know that the human brain constructs a phenomenal self-model (PSM)—a high-resolution simulation of a subject embedded in a world. Crucially, this model is transparent: it does not represent itself as a model. Instead, it is lived-through as reality; it is the very content of the model.

We know then, from this, arises the illusion of a subject. But the illusion is not like a stage trick seen from the outside. It is a hallucination without a hallucinator, a feedback system in which the representational content includes the illusion of a point of origin. The brain simulates an experiencer, and that simulation becomes the center of gravity for memory, agency, and attention.

Perhaps the most disorienting implication about subjectivity is this:

*The certainty of being a subject is itself a feature of the model*

what might bridge this gap and explain how the brain produces this persistent, centered “I-ness”? How can a purely physical substrate generate the transparent phenomenological immediacy of first-person subjectivity? *HOW does* the brain's processes create a transparent-phenomenal self? the mechanism of the existence of such *transparency* without resorting to epiphenomenalism(dualism)?",1752380805.0,computerscience,STEM
1lxws8d,Struggling to understand this proof of cost-optimality for A* search,"https://preview.redd.it/rijwxugx0fcf1.png?width=893&format=png&auto=webp&s=0b4d2af4fed5862b20e6ba105cf9c204b2e55811

I'm struggling to deeply understand this proof. Firstly, if we start with assuming that n is a node on the optimal path, then how have we then assumed f(n) >  C\*? n is just a node on the path with cost C\*, so how could the evaluation function for n f(n) be greater than C\*? Or is this just the blanket assumption we start with that we're trying to disprove?

 Secondly, for an admissible heuristic h(n), it feels weird that the authors have written h(n) <= h\*(n) instead of h(n) = h\*(n). Wouldn't an admissible heuristic h(n) one that refer to the optimal path cost h\*(n) by definition? The <= looks weird to me because I don't seem to register how h(n) might be lower than h\*(n) I guess.",1752314336.0,computerscience,STEM
1lxqcpg,Numpy Usage for A*?,"I am implementing the A\* algo pseudocode from wiki([A\* search algorithm - Wikipedia](https://en.wikipedia.org/wiki/A*_search_algorithm#Pseudocode))

I know that a priority queue or a Fibonnaci heap is often recommended for Djikstra's or A\*, but I also read that Numpy is heavily parallelized. Is there any way to use Numpy arrays as the maps for fScore and gScore, such that it would be faster than using a PQ or heap for each loop? The reason I ask is that when putting all my points in a hash map for fScore and gScore, it takes a long time, and I assume inserting in a PQ or heap would be longer.

Thank you!",1752290536.0,computerscience,STEM
1lxvys5,"Is it possible to describe cybersecurity concepts purely in technical terms, without relying on real-world objects?",,1752310930.0,computerscience,STEM
1lxo2lz,How to write a CS research paper?,I've written a couple of research papers earlier (not based on CS) but I'm genuinely interested in writing a CS research paper. I read articles and watched some youtube videos but neither of them seemed to be helpful.,1752283413.0,computerscience,STEM
1lvz8o9,"Is there any name for a situation like this, where we can't make computers more advanced","I just wonder if there is a name for a situation where we reach an ultimate limit to CPU speeds and power, simply because there is nothing smaller to make computer components out of. Transistors kept shrinking and shrinking as you could make them smaller. In theory the smallest we could make P type silicon and N type silicon is one atom of silicon doped with an atom of another element to make it P or N type. Even this is not possible because of quantum tunneling, but if it was, what then?

I know about quantum computers, but they are not general purpose like a PC CPU.",1752108047.0,computerscience,STEM
1lvsb8o,A new attempt at human centric vision.,"Introducing Druma One our humble attempt at building human centric vision one keyframe at a time. This enables a new direction towards some of the most pressing problems in vision like action recognition, gesture recognition, object detection, SLAM, 3D mapping with edge compute.

  
Please find the link here.

[https://github.com/Druma-Tech/Druma-One](https://github.com/Druma-Tech/Druma-One)",1752090336.0,computerscience,STEM
1lvi3zu,What should I study on my own?,"I'm in my first year of Computer engineering and I'm currently learning C++.
Once I'm familiarized enough with it, what else should I start learning? Advice online while plentiful is also very confusing as there's not a clear definite answer. 
I'd like to eventually develop an Android app, but that can wait if there's something more important to learn first. ",1752065850.0,computerscience,STEM
1lvblvc,Deleting things,"I’m having trouble understanding that the things we download take up space in a measurable amount, but when you delete it, it’s completely gone from your computer? 

Is that true? Does the data or code you downloaded go somewhere outside of your computer when you delete it? Or does it stay in a smaller packet of some sort? Where does it go?",1752042130.0,computerscience,STEM
1lve460,"Reading Arora and Barak, what to do for a hands-on learning activity?","For background I have studied (a good chunk) of CLRS and Parts 1 and 2 of Sipser (automata and computability). 

What could i do on the side while studying Arora and Barak? Maybe something research oriented…? I’m unsure so i’d be happy to get some ideas!

Papers like this seem really fun: https://doi.org/10.1016/S0925-7721(99)00017-6",1752052283.0,computerscience,STEM
1luelmz,What language did your CS courses start you off with and why?,Would you have preferred it to be different?,1751945850.0,computerscience,STEM
1luz5xj,"Tech teachers! Do you let your students tinker with CPUs, old technology, or other tech related gadgets? Rural teacher with a low budget looking to encourage learning the components of technology.",,1752005978.0,computerscience,STEM
1luyroq,Any good CA&CO course videos,"Hi! I recently started learning Computer architecture and organization but I literally can't keep up because it's a lot and my finals are in a month. I'm the type of person who understands from practical lectures so theory/text lectures are a bit difficult for me to absorb.
 
I was wondering if there's any good free course videos that explains step by step and doesn't make me feel like I'm listening to someone explain in a whole new different language? Ty!",1752005052.0,computerscience,STEM
1luwta2,Anyone willing to explain the OSI model to me?,"Like I don't know if I'm dumb or what but I've read multiple articles and watched a few vids and they either are shallow or just convoluted. I like to try and make analogies so I can understand them well. I guess I will try to explain what I know and how I understand it and what issues I have.

THE PHYSICAL LAYER

as the name suggests it's all about the physical parts. Cables, how they connect to devices, what pins do what, what is their bandwidth, what is the rate of transmission, or they don't need to be cables, they can be signals. In a way it's a medium trough which we pass on the data, and in essence, the data we pass is in bits, everything else is an abstraction. It also is responsible for reassembling the bits I guess, because you get them in a stream sort of. So the core functionalities are transmitting the signal and reassembling it. I guess if the physical layer were a person In my head I don't know why I imagine them flicking a light on and off or a laser to send messages. So they are in charge of turning it off and on, they control the speed at which they do it and at the other end they are also in charge of writing the signal down on paper (reassembling).

DATA LINK LAYER

""The data link layer is responsible for the node-to-node delivery of the message"", ammm isn't the first layer responsible for that? Also what do you mean responsible for delivery. If the layer were a person would they get the message from the first guy (the signals written on paper) and give it to the person that the message was meant for? Sort of like a multiplexer, switching the channels so the message goes to the right person. As I understand its responsible for communication in a network, not across them. This layer also works off of MAC addresses and it does error control. The MAC addresses are in the header and the error control is in the tail of the frame. Now I assume because it's above the physical layer, it tells the physical layer who to send the message too (what mac address) 

THE NETWORK LAYER

""The network layer works for the transmission of data from one host to the other located in different networks"" doesn't the first layer do this? It feels like every layer is transmitting something. It's the router layer I guess because routers are the main actors here. 

""It also takes care of packet routing i.e. selection of the shortest path to transmit the packet, from the number of routes available."" so it's basically pathfinding. I guess if it were a person they would turn the laser pointer towards the location where we want to send the message to. I read that it has routing tables which are kind of like maps but the thing that I don't get is, it's basically a map of neighbours. It works off of IP addresses which in a network are private so it needs to switch to a public IP and find the path. I guess it sends out signals to other devices to ask if they know where to go. But this feels inefficient. Like I said it's sending a message to the neighbours to ask for help, and those neighbours send messages to their neihbours (if they dont know where the location is) and that repeats but I dont know how much. Here the unit is the packet and It's said that the packet encapsulates the frame but isn't it the other way around? The packet is passed to the 2nd layer so does the second layer just wrap the packet up into a frame or he puts the frame in the packet?   
  
THE TRANSPORT LAYER  
  
""The data in the transport layer is referred to as **Segments**. It is responsible for the end-to-end delivery of the complete message. The transport layer also provides the acknowledgment of the successful data transmission and re-transmits the data if an error is found."" isn't the acknowledgment protocol specific? And again ""responsible for delivery"" girlll how, if the first layer is a truck driver carrying packets and the third layer tells him the directions, how is this layer responsible for delivery? Like the possible problems are, the trucks breaks so that's layer 1 issue or they don't know where to go which is layer 3 issue. ""also implements **Flow and error control** to ensure proper data transmission. It also adds Source and Destination port number in its header"" again don't other layers control the flow and why are 3 different layers adding the port ip address and MAC address, it would be like if I wrote the number on a envelope, then passed it on to the next person who would write the street name, and then passed it on to an another person who would write the city name and country. 

THE SESSION LAYER

""Session Layer in the OSI Model is responsible for the establishment of connections, management of connections, terminations of sessions between two devices."" is a connection a mutually acknowledged one? Because some protocols don't expect acknowledgments. Also doesn't the first layer do the connection thing. If this layer were a person, would they be sitting next to the first person who is flicking the light switch or laser and looking at their stopwatch to see how long the session is lasting or maybe noting down if there was an acknowledgement?

  
THE PRESENTATION LAYER

""The data from the application layer is extracted here and manipulated as per the required format to transmit over the network."". So they are in essence, packing the mail or whatever, encrypting it etc. Seems simple enough.

THE APPLICATION LAYER

""At the very top of the OSI Reference Model stack of layers, we find the Application layer which is implemented by the network applications. These applications produce the data to be transferred over the network."" So they are basically ur pen and paper, u write stuff down which begins the whole chain.

  
I guess these last few seem okay but the first 4 seem to be doing a lot of the same thing. I guess I'm looking for some analogy to tie them all together, because lets say I was given the task of writing something down and sending it to someone. Lets say I know the name of the person, so the first step is to write the letter (application layer right?) then I have to pack it in an envelope, write down the details who it should go to, where it came from etc , or maybe if its an object i have to pack it in a box with bubble wrap etc (presentation layer). Then I have to figure out where to go, and lets say i dont have a google map so I have to go around asking ppl in the neihbourhood for directions, I guess that is the Network layer, but while im going on the road, its like im on the physical layer right. Does the network layer wait to get the full response and then sends out the packet, or it sends out packets and they change direction as they get more info on where to go? And I guess there is the part of respecting street signs and traffic (flow) so that's the 2nd layer or idk half of them since they all do some flow control apparently. ",1752000506.0,computerscience,STEM
1lt45do,You Don't Need to Understand Everything at Once and That's the Point.,"One thing I wish more people said out loud in CS: it’s okay not to understand everything right away. In fact, you won’t. Not even close.

There’s a myth that if you don’t instantly “get” recursion, pointers, or Big O, you’re not cut out for computer science. But honestly? The reality is more like this: you’ll loop back to the same topic five times over the years, and each time it makes a little more sense.

Most of CS is layered knowledge. You learn enough to move forward and later, when you revisit, you fill in the gaps.

When I was just starting, I struggled with operating systems. I read about scheduling algorithms and memory paging and thought, “Wow, this is way over my head.” Five years later, I was debugging race conditions in multithreaded code and those OS concepts finally clicked. But I had to live with the confusion for a long time before that.

So if you're a student or a self-learner and you're feeling overwhelmed:  
→ That's normal.  
→ You're not behind.  
→ You’re doing fine.

Computer science isn't a race. It's more like building a giant, complex mental map. And every time you learn something new, another piece of that map lights up.

Be patient. Take breaks. Ask “dumb” questions. Go deep on what interests you, and let the rest sink in slowly.

And above all, keep going.",1751816190.0,computerscience,STEM
1lsscuj,I finally understand what a linked list is.,"I seen implementation of linked lists many years ago but never understood what it is, now in my graduate class I finally understand what a linked list is, it is essentially multiple instances of a class referring to each other through their class attributes in an orderly fashion thus forming a linked list. Am I right? 

Edit: I meant in the title how to implement a linked list, not what it actually is, sorry about the confusion. ",1751775269.0,computerscience,STEM
1lsrk52,What would the potential applications in computational biology be if the dynamic optimality conjecture was solved?,What would it mean for computational biology if it was proven true and what would it mean for computational biology if it was proven false? ,1751772509.0,computerscience,STEM
1lsvuih,Does customization reduce OS performance?,,1751788676.0,computerscience,STEM
1ls3mom,Question from a newbie,"Computers and electricity have always seemed like magic to me (im only 29 😬) but ive recently tried to make myself learn how it all works and i have a question about transistors. From what ive found the current iphone for instance uses a 3nm transistor which is only about 15-20 silicone atoms across. According to Moore’s Law, transistors should shrink by half every 2 years so theoretically we could have 3 atom transistors (correct me if im wrong but 3 seems to be the logical minimum based on my understanding of the fact you need an n-type emitter/p-type base/n type collector) in 6 years. What happens when we get to that point and cant go any smaller? I read a little about electron tunneling but am not sure at what point that starts being a problem. Thanks for any insight and remember im learning so explain in baby terms if you can 😂",1751698071.0,computerscience,STEM
1lrn84g,"Outside of ML, what CS results from the 2010-2020 period have changed CS the most?",I am particularly interested in those that have real-world applications.,1751647151.0,computerscience,STEM
1ls1i9b,Binary search and mid value,"    gemnum = 25
    low = 0
    high = 100
    c = 0
    if gemnum == (low + high)//2:
        print(""you win from the start"") 
    else:
        while low <= high:
            mid = (low + high)//2
            print(mid)      
            if mid == gemnum:
                print(c)
                break
            if mid > gemnum:
                high  = mid
                c = c + 1
            else:
                low = mid
                c = c + 1

The above finds gemnum in 1 step. I have come across suggestions to include high  = mid - 1 and low = mid + 1 to avoid infinite loop. But for 25, this leads to increase in the number of steps to 5:

    gemnum = 25
    low = 0
    high = 100
    c = 0
    if gemnum == (low + high)//2:
        print(""you win from the start"") 
    else:
        while low <= high:
            mid = (low + high)//2
            print(mid)      
            if mid == gemnum:
                print(c)
                break
            if mid > gemnum:
                high  = mid - 1
                c = c + 1
            else:
                low = mid + 1
                c = c + 1

Any suggestion regarding the above appreciated.

Between 0 and 100, it appears first code works for all numbers without forming infinite loop. So it will help why I should opt for method 2 in this task. Is it that method 1 is acceptable if gemnum is integer from 0 to 100 and will not work correctly for all numbers in case user enters a float (say 99.5)?",1751689869.0,computerscience,STEM
1lq7zzu,EILI5: What exactly is the practical point of quantum computers?,"I know I’m missing the bigger picture, which is why I’m asking, but right now, I can’t wrap my mind around what the practical uses of a quantum computer could be. Maybe it’s because I’m not a physicist or mathematician, but what are quantum computers doing that regular super computers can’t already do? Is this something that’s only relevant to physicist and mathematics, or could have a more practical application in the real world down the line? ",1751491978.0,computerscience,STEM
1lpqpmm,Why is the Unicode space limited to U+10FFFF?,"I've heard that it's due to the limitation of UTF-16. For codepoints U+10000 and beyond, UTF-16 encodes it with 4 bytes, the high surrogate in the region U+D800 to U+DBFF being multiples of 0x400 from 0x10000, low surrogate in U+DC00 to U+DFFF being 0x000 to 0x3FF. UTF-8 has extra 0xF5 to 0xFF bytes so only UTF-16 is the problem here.

My question is: why does both surrogates have to be in the region U+D800 to U+DFFF? The high surrogate has to be in that region as a marker, but the low surrogate can be anything, from U+0000 to U+FFFF (I guess there are lots of special characters in the region but the text interpreter can just ignore that, right?) If we take full advantage, the high surrogate could range from U+D800 to U+DFFF, being multiples of 0x10000, making a total of 0x8000000 or 2\^27 codepoints! (plus the 2\^16 codes of the BMP) So why is this not the case?",1751445804.0,computerscience,STEM
1lpyx55,LLM inquiry on Machine Learning research,"Realistically, is there a language model out there that can:

* read and fully understand multiple scientific papers (including the experimental setups and methodologies),
* analyze several files from the authors’ GitHub repos,
* and then reproduce those experiments on a similar methodology, possibly modifying them (such as switching to a fully unsupervised approach, testing different algorithms, tweaking hyperparameters, etc.) in order to run fair benchmark comparisons?

For example, say I’m studying papers on graph neural networks for molecular property prediction. Could an LLM digest the papers, parse the provided PyTorch Geometric code, and then run a slightly altered experiment (like replacing supervised learning with self-supervised pre-training) to compare performance on the same datasets?

Or are LLMs just not at that level yet?",1751470341.0,computerscience,STEM
1lp503l,Any feedbacks for this insertion sort visualization?,"Hi everyone! I need to gather some insights.

What do you guys think about this video? Are there any feedback or opinions? Do you guys understand it quick? Any insight is much appreciated!

[Insertion Sort Visualization](https://youtube.com/shorts/N44AwkQjeCU?si=26yY1zW8WEdLkHaY)",1751384550.0,computerscience,STEM
1lps28d,Is python really this big?,"https://preview.redd.it/uvwuzkadrfaf1.png?width=1309&format=png&auto=webp&s=cbdb2eb61ba68f8a2035dfaa60c179b9756d8628

I thought rust would be bigger overall ngl",1751451120.0,computerscience,STEM
1loc8lc,Learning CS using OSSUs roadmap vs roadmap.sh,"So I am interested learning about CS and after some researching on how I can learn by myself I've stumbled upon OSSU [https://cs.ossu.dev/](https://cs.ossu.dev/). I have also found [https://roadmap.sh/computer-science](https://roadmap.sh/computer-science). What are the differences and which one would be better to stick to? OSSU honestly seems like it's more thought out and gives you a simpler, step-by-step approach on what to learn first and then second etc. And when first looking at [roadmap.sh](http://roadmap.sh) it kind of looks like it's giving you a ton of stuff and throws them at you. It definitely doesn't look as simple to follow as OSSU in my opinion, and I think that you can get overwhelmed. In OSSU you start with CS50 which gives you an introduction and I have just started and on week 0 but I gotta say, I am already liking this professor, he is really a good explainer and CS50 just seems like a really good intro to start learning CS.

  
Anyways what do you guys think about these options, are they solid? And maybe you guys have some other resources to learn CS. I would love to hear those.",1751301368.0,computerscience,STEM
1lpe8im,Is it hard to read your teammates code? Could source code maintained in natural language improve this?,"Imagine you could write code in natural language aka ""natural code"", and you ""compile"" the natural code to traditional computer code using an LLM. It minimally updates the computer code to match changes MADE to the natural code, then compiles that using a traditional compiler. The coder can then see both kinds of code and links between the two.  Alternatively you do this on a per function basis rather than per file.

Note that though coders write in natural language, they have to review the updated code similar to git diffs to ensure AI understood it correctly and give them a chance to prevent ambiguity issues.

Do you believe that this would help make it easier to write code that is easier for your teammates to read? Why or why not?",1751405847.0,computerscience,STEM
1lld1at,What sort of computer could be the next generation that could revolutionize computers?,"The evolution of computers has been from analog (mechanical, hydraulic, pneumatic, electrical) and then a jump to digital with 5-7 generations marked by the transitions from vacuum tubes to transistors, transistors to integrated circuits and this one to VLSI. 

So if neuromorphic, optical and quantum computing all can only be for special purpose, then what technology (although far to be practical for now) could be the next generation of general purpose computers? Is there a roadmap of previus technologies that need to be achieved in classical computers in order for the next generation to arrive?",1750975343.0,computerscience,STEM
1lkrxuo,Deterministic Finite Automata,"Hi, new CS student here, recently learnt about DFAs and how to write regular expressions and came across this question:

Accept all strings over {a, b} such that there are an even number of 'a' and an odd number of 'b'. 

So the smallest valid string is L = {b, ...}. Creating the DFA for this was simple, but it was the writing of the regular expression that makes me clueless. 

This is the solution I came up with:
RE = {(aa + bb + abab + baba + abba + baab)* b (aa + bb + abab + baba + abba + baab)* + aba}

My professor hasn't done the RE for this yet and he said my RE was way too long and I agree, but I can't find any way to simplify it.

Any advice/help is welcome :D",1750915408.0,computerscience,STEM
1lkd3pf,What can people see when you use https:// instead of http://?,"From what I understand, people using the same router can generally see the domain name, but not the individual pages.

However, if I visit Tumblr with an address like: [https://pusheen.tumblr.com](https://pusheen.tumblr.com), will people see the ""pusheen"" part too?",1750875196.0,computerscience,STEM
1lleyi3,Is optimization obsolete with quantum computing?,"Say for instance in the distant future, the computers as we have today transition from CPU’s to QPU’s, do you think a systems architecture would shift from optimization to strictly readable and scalable code, or would there be any cases in which optimization in the “quantum world” would be necessary like how optimization today would be necessary for different fields of applications. ",1750980428.0,computerscience,STEM
1lkbe79,"Reading papers, understanding papers, taking proper notes","1. How to read a paper?

2. What steps should I follow to properly understand a paper?

3. How to take proper notes about the paper? Which tools to use? How to organize the extracted information from the paper?

4. How to find new research topics? How to know that this fits my level (Intelligence, Background Knowledge, Computational Resources, Expected Time to complete the work etc.)? Is there any resources to find or read recent trending research papers?

5. Anything you want to add to guide an nearly completed undergrade student to get into the research field.",1750871344.0,computerscience,STEM
1lkztx5,can someone list languages between python and machine code in order of complexity.,"I am trying to make list in a top down style of high level to low level programming languages for a book I am writing. In my book python is the simplest and highest level program language. The list end with machine code, the absolute lowest level of programing that I know of.  ",1750943648.0,computerscience,STEM
1ljl1fp,"Turing’s On Computable Numbers, with an Application to the Entscheidungsproblem (1937) considered Alan Turing most significant work sold at Hansons (UK) auction for GBP 200,000 ($269,308.60) on June 17, as reported by RareBookHub.com","This sale titled: The Alan Turing Papers: The Collection of Norman Routledge (1928-2013), Fellow Mathematician & Personal Friend of Alan Turing. Catalog notes comment: Unsigned but the author's personal copy, given by Turing's mother to Norman Routledge, also notes: “Turing's most significant work. The most famous theoretical paper in the history of computing. The foundation of computer science & modern digital computing. The birthplace of the stored program concept used by almost all modern-day computers. This is the paper that introduced the world to the idea of a ""universal computing machine"", which, despite the model's simplicity, is capable of implementing any computer algorithm. ""Effectively the first programming manual of the computer age."" \[COPELAND, Jack. The Essential Turing, pp. 12-13, Oxford: Clarendon Press, 2004\]. The Turing Archive \[AMT/B/12\] ",1750794536.0,computerscience,STEM
1lk3jh1,Learning about blockchain,"Hi ,  i work as a research assistant and my professor’s comping research work is a blockchain based solution and he asked to to learn and understand blockchain. 
I do have some basic knowledge about blockchain and how it works but i feel like it’s not enough to work in a research related in this area , so if you guys could please provide me with some good resources to get enough theoretical and practical knowledge within a month or two. I know this might sound impossible , but i just need enough knowledge to start drafting the theoretical aspects of the solution.
",1750852142.0,computerscience,STEM
1ljgv5q,difference between a program having a built in restart button vs powering off and powering on?,"im having a debate between me and a friend cuz we are trying to solve a meta quest 3 issue, what is the difference between an os having a built in restart button which shuts off the os then turns itself back on to re initialize itself, and powering down the device, waiting 1 minute for the ""electricity to disipate"", then turning back on the device, to reinitialize the os. because to me those seem functionally identical",1750785101.0,computerscience,STEM
1limnjp,Can computers forget things in their memory?,"Can computers forget things in their memory and if so how can it be prevented? I hear computers store memory through electron traps, but electrons have a way of moving about and seem difficult to contain so wouldn't memory change on it's own after time?

This scares me because I love to collect all the computer games I've played and many of them you spend dozens of hours building a saved game. It would break my heart to lose a game save I spent hours working on.",1750699466.0,computerscience,STEM
1ljl199,hi reddit im looking for info on rom and eeprom,"hey reddit i love sceince and lately im checking out rom and eeprom i love the possibility of a customizable computer using aka eeprom but i have few question do you have any idea of how the transistors in eeprom work do they use multiple electrons or just 1 to repersent 1 and 0 does eeprom use address finding like ram does also do you have access to any articles that talk about this and how the atomic structure of this works.  
 Also moderators if this is against any rules ill happily re change just contact me quickly and quietly. ",1750794526.0,computerscience,STEM
1lip4yu,Contributing idle compute power to science?,Is it possible to contribute personal idle compute power to science?,1750705094.0,computerscience,STEM
1li63ij,What are the odds that P = NP will actually result in faster calculations in any practical sense?,"Is it even feasible that if P = NP that a polynomial solution for an NP problem scales with a polynomial time complexity that will be pragmatically useful for speeding up technological innovations? Or is it way more likely in the small chance that P = NP that the polynomial time algorithms needed to solve NP problems will be so large that they won’t have much practical applications in advancing technology? In the latter case I think only the math used to solve the problem will have any practical real world applications. 

ETA: For clarification, I thought of these questions after reading a recent post on this subreddit: https://www.reddit.com/r/computerscience/s/HpBSrgHy7f",1750648108.0,computerscience,STEM
1lhvdwt,A formal solution to the 'missing vs. inapplicable' NULL problem in data analysis.,"Hi everyone,

I wanted to share a solution to a classic data analysis problem: how aggregate functions like AVG() can give misleading results when a dataset contains NULLs.

For example, consider a sales database :

Susan has a commission of $500.

Rob's commission is pending (it exists, but the value is unknown), stored as NULL.

Charlie is a salaried employee not eligible for commission, also stored as NULL.

If you run SELECT AVG(Commission) FROM Sales;, standard SQL gives you $500. It computes 500 / 1, completely ignoring both Rob and Charlie, which is ambiguous .

To solve this, I developed a formal mathematical system that distinguishes between these two types of NULLs:

I map Charlie's ""inapplicable"" commission to an element called 0bm (absolute zero).

I map Rob's ""unknown"" commission to an element called 0m (measured zero).

When I run a new average function based on this math, it knows to exclude Charlie (the 0bm value) from the count but include Rob (the 0m value), giving a more intuitive result of $250 (500 / 2).

This approach provides a robust and consistent way to handle these ambiguities directly in the mathematics, rather than with ad-hoc case-by-case logic.

The full theory is laid out in a paper I recently published on Zenodo if you're interested in the deep dive into the axioms and algebraic structure.

Link to Paper if anyone is interested reading more: [https://zenodo.org/records/15714849](https://zenodo.org/records/15714849)

I'd love to hear thoughts from the data science community on this approach to handling data quality and null values! Thank you in advance!",1750618053.0,computerscience,STEM
1lgqdug,"Tips on self-studying from textbooks, and how the heck can I verify my solutions?","Hello. Any tips on self-studying textbooks? Especially the theoretical ones.  
The biggest challenge for me is to validate my solutions. I'm currently studying the [CLRS](https://mitpress.mit.edu/9780262046305/introduction-to-algorithms/) book, and it's pretty dang hard to find solutions online and verify my own, especially since most of the exercises and problem sets involve proofs, and those ones are hard to validate.  
This isn't about CLRS only. Most of the textbooks don't have solutions for the exercises.  
Most of the solutions on the internet are either incomplete or done by individual contributors, which I can't validate.  
It'd be great if you could give me any tips on this. Especially on proof validation, as proofs vary greatly and more than one solution can be correct. Thanks.",1750489183.0,computerscience,STEM
1lg1dhw,C# (Help/Advice),I am 18 and will start CS at Uni this September. I’ve started learning C# with Alison.com and have made notes on paper when working through the videos to build my understanding. Am I doing it correctly? I want to learn the concepts before going knee deep into starting my own projects.,1750418036.0,computerscience,STEM
1lfi1c6,Computing with geometry?,,1750357278.0,computerscience,STEM
1lf0oqf,"Saved Alan Turing papers sold at auction in Etwall for £465,400",,1750303535.0,computerscience,STEM
1lezp5z,How are all NP problems reducible to NP-Complete?,"I know that by definition, NP-Complete is the set of problems that is in NP and can be reduced to from every other NP problem. However, I can't seem to wrap my head around how so many different problems can all be reduced to a single problem.",1750300466.0,computerscience,STEM
1lflyrr,Has anyone seriously attempted to make Spiking Transformers/ combine transformers and SNNs?,"Hi, I've been reading about SNNs lately, and I'm wondering whether anyone tried to combine SNNs and transformers. And If it's possible to make LLMs with SNNs + Transformers? Also why are SNNs not studied alot, they are the closest thing to the human brain and thus the only thing that we know that can achieve general intelligence. They have a lot of potential compared to Transformers which I think we reached a good % of their power.",1750366918.0,computerscience,STEM
1lelr6f,Theoretical Computer Science,"I have always been very curious about the theoretical approach to CS but never really got the guidance to it(currently a pre-uni aspiring to study CS Theory) as most of the CS majors i know often expects me to learn only the tools and the developing of sites, softwares etc. whereas I want to learn the math and science behind those magical rocks that builds up the modern society",1750264546.0,computerscience,STEM
1lewo4c,Heuristic for A* For Terrain Cost,"I am working on a problem which involves using A\* for finding the optimal route on a terrain of varying slope. I am representing the terrain as a network where each point is a node and adjacent points are connected by an edge. The cost of a path between two points is the line integral of the absolute values of the slopes of the path. Currently the best heuristic function I can think of is the net slope between the current point and the end goal, but I was wondering if anyone can think of or has used a heuristic function which is, on average, closer to the cost function between the current function and the goal.",1750291462.0,computerscience,STEM
1ldm7of,My teacher's algorithms make no sense to me,"Our teacher is making us learn like 80 algorithms for his exam and half of them are just straight up gibberish to me. Can anyone tell me if this makes any sense to them? Just want to know if this course is literal bs or not.

https://preview.redd.it/3jse7088hh7f1.png?width=655&format=png&auto=webp&s=173567bb04675326cc7a4ec86d4e2c769f223647

",1750164406.0,computerscience,STEM
1lc6uyp,What is the theory behind representing different data structures using arbitrary length integers?,"I am not a student of CMU. I just found out this interesting problem while surfing the web. 

Here is the problem called Integer Data Structures: [https://www.cs.cmu.edu/\~112-f22/notes/hw2-integer-data-structures.html](https://www.cs.cmu.edu/~112-f22/notes/hw2-integer-data-structures.html)

I want to know the source of this problem. Does this come from information/coding theory or somewhere else? So that I can read more about it.",1750010917.0,computerscience,STEM
1lbthop,Graphics cards confuse the hell out of me :(,"I've been getting into computer science as of late and i've pretty much understand how cpus work, i even built a funnctioning one in minecraft,  just as a test. but anyways my problem is that I can't find an in depth description of how a gpu works. I can only get surface level information like how they perform simple arithmetic on large amounts of data at once. thats useful info and all but i want to know how it renders 3d shapes? I understand how one might go about rendering shapes with a cpu, just by procederally drawing line betweens specified points, but how do gpus do it without the more complex instructions? also what instructions does a gpu even use? Everything i find just mentions how they manipulate images, not how they actually generate them. I dont expect a fully explaination of exactly how they work since i think that would be a lot to put in a reddit comment but can someone point out some resource i could use? preferably a video since reading sucks?

PS Ive already watched all the Branch education videos and it didnt really help since it didnt really go through the actual step by step process gpus use to draw shapes. i want to know what kind of data is fed into the gpu,, what exactly is done with it, and what it outputs?",1749968322.0,computerscience,STEM
1lbvd5q,Exploring Emerging Areas in Computer Science,"Hey everyone,
I’ve been reading up on different areas of CS and I’m curious what emerging fields people find most exciting right now from a research and theoretical perspective.

Whether it’s new developments in machine learning, distributed systems, algorithms, programming language design, computer vision, or even newer experimental topics — I’d love to hear what areas you think are showing a lot of potential for innovation.

Mainly just trying to broaden my understanding of where CS seems to be heading in the next few years. Appreciate any thoughts or recommendations for areas worth diving into!
",1749975854.0,computerscience,STEM
1lbceol,Cyclic Tag System,"I have heard that cyclic tag systems (CT) are Turing complete. I am concerned with a specific instance of CT wherein the tape or stack or whatever you call it starts at as a single one. For those unaware of what CT is (or those who cannot understand my very horrible description):

You have a program consisting of 0's, 1's and semicolons. You also have a tape/stack of 0's and 1's. The version I'm concerned with features this stack starting as a single 1. You read the program cyclically, going back to the first symbol when you reach the last. Each time you read a symbol, you modify the stack as follows: if the symbol is a semicolon, unconditionally delete the first symbol of the stack. If the symbol is a 0 or 1, check if the first symbol of the stack is a 0 or 1. If and only if it is a 1, attach a 0/1 to the end of the stack (attach 0 if the symbol on the program is 0 and attach 1 if the symbol on the program is 1). Otherwise, do nothing and move on to the next symbol of the program.

Anyway, I have heard that this restricted version of CT where the starting stack is always just a single one is still Turing complete; ergo, for any given Turing machine, there exists a CT program that emulates it. My question is this: what is an upper bound for the length of a CT program required to emulate a Turing machine of n states? I am not talking about the computation TIME upper bound to simulate the Turing machine; I am talking about the program LENGTH upper bound.

EDIT: I think I might have found an answer at [https://www.cstheory.org/meetings/sp24/tag/slides.pdf](https://www.cstheory.org/meetings/sp24/tag/slides.pdf), which details a way of converting any Turing machine into a cyclic Tag system. However, comments and additional information is still wanted.",1749917632.0,computerscience,STEM
1l8ynq8,CS new frontier,"As a relatively new CS student, I'm thinking a lot about where the field is headed. It feels like machine learning/deep learning is currently experiencing massive growth and attention, and I'm wondering about the landscape in 5 to 10 years. While artificial intelligence will undoubtedly continue to evolve, I'm curious about other areas within computer science that might see significant, perhaps even explosive, growth and innovation in the coming decade.

From a theoretical and research perspective, what areas of computer science do you anticipate becoming the ""next frontier"" after the current ML/DL boom? I'm particularly interested in discussions about foundational research or emerging paradigms that could lead to new applications, industries, or shifts in how we interact with technology.",1749662843.0,computerscience,STEM
1l95qw7,Comparing two adjacency matrices for graph equality,"Hello folks , do you know any algorithm(or any implementation in any programming langage) to compare two adjacency matrices for graph equality?",1749679779.0,computerscience,STEM
1l8gu8j,The Beauty of Data Conversion.,"The image is a 3 seconds audio of the Piano C Key.



Its being converted from WAV audio sampling points into Sound Partials that are stored as 2D NURB curves.



Very Nice for noise filtering and audio editing.



Short-Time Fourier Transform (STFT) was used for NURB path detection. The parameters for conversion were based on time cell size, minimal NURB path length, and signal energy minimum and maximum limits.

  
",1749607138.0,computerscience,STEM
1l8m3gv,What are some really strong ways to learn combinatorics and counting?,"Hi all,

I’ve recently realized how important combinatorics and counting techniques are—not just in competitive programming, but also in algorithms, probability, and even real-world software problems like optimization, hashing, and graph theory.

That said, I feel like most resources either jump straight into formulas without intuition, or drown you in puzzles.

What are some of the most effective strategies or resources you’ve used to deeply learn combinatorics and counting?
For example:

Are there any books that explain the ""why"" behind formulas like permutations, combinations, pigeonhole, inclusion-exclusion, etc.?

Feel free to share really good problem sets

Did visual tools or interactive simulations help?

How do you balance theory vs practice here?


I'd especially appreciate tips that go beyond just memorizing formulas—I'm looking to really internalize how to think combinatorially.

Thanks in advance!",1749625390.0,computerscience,STEM
1l8by05,Collatz as Cellular Automata,,1749593317.0,computerscience,STEM
1l86h59,Need help understanding this,"As the title says, I have trouble understanding why y-x+1 gives the number of descendants. Could someone explain this to me, ideally with an example? Thanks!! ",1749580369.0,computerscience,STEM
1l7zcrn,Books for forensics,"Hi Everyone 

Does anyone knows a good book on Cyber forensics ?

",1749563709.0,computerscience,STEM
1l864bb,What kind of research is going on in computer networks/network security?,,1749579555.0,computerscience,STEM
1l7pvv2,why isn't floating point implemented with some bits for the integer part and some bits for the fractional part?,"as an example, let's say we have 4 bits for the integer part and 4 bits for the fractional part. so we can represent 7.375 as 01110110. 0111 is 7 in binary, and 0110 is 0 * (1/2) + 1 * (1/2^2) + 1 * (1/2^3) + 0 * (1/2^4) = 0.375 (similar to the mantissa)

",1749529977.0,computerscience,STEM
1l7hb04,"Inside Naval Computing History: Mechanical, Analog, and Early Digital Systems in Action","This image shows a Cold War-era Naval Tactical Data System (NTDS) console, likely from a destroyer or cruiser retrofitted in the 1960s–1970s. This system represented the digital revolution of naval warfare, where electromechanical and analog computers like the Mark 1A and TDC began to be replaced with digital computers and operator consoles.",1749505055.0,computerscience,STEM
1l6q284,"These WWII Machines Solved Real-Time Trig with Gears, Not Chips","Look inside the brain of a WWII submarine: This is a Torpedo Data Computer (TDC), a mechanical analog computer that helped U.S. Navy subs calculate real-time intercepts for torpedoes. No screens, no code — just gears, cams, and sheer ingenuity.",1749425313.0,computerscience,STEM
1l71i61,What are some good resources to learn automata theory?,,1749465676.0,computerscience,STEM
1l62bis,Do yall actually like programming?,Anytime I talk to someone online or in person about comp sci they just complain about it I’m I the only one who genuinely likes programming or I’m I just a masochist,1749352124.0,computerscience,STEM
1l6b1ic,"A collection of knowledge cards on basics of boolean logic, low level programming (RISC-V assembly) and computer architecture","Hey there! If you are interested in learning low level programming (assembly), boolean logic and processors, I’ve just finished creating an Anki deck focused exactly on that.

For those who don't know, Anki is a popular app for spaced repetition learning, but you can also use it as a knowledge database, if you are not into that. Inside this collection of cards you’ll find:

* Explanations of RISC-V processor, calling conventions, and assembly instructions (with SVGs and HTML/CSS embeds for graphics and videos).
* Sections on boolean logic and finite-state machines to build a solid digital logic foundation.
* Exercises, 3 interactive CPU simulators from the web and lots of reference tables.

[A preview of a few of the cards in the deck](https://preview.redd.it/9gn951wd5p5f1.png?width=1859&format=png&auto=webp&s=4bcedbc989fc90c10e5ac6107b7d66a6a90bf19c)

Here's the link: [https://ankiweb.net/shared/info/1737020042](https://ankiweb.net/shared/info/1737020042)

I hope you'll find this resource helpful, it’s completely free to download and use. Let me now if you have any feedback! 😊",1749385899.0,computerscience,STEM
1l6dh66,i made a mathematics software in computer science which can solve mathematics. how can i publish this research so that it can be known to more people ?,"how to publish research in computer science python programming. i can make a library also on it and provide documentation of it. but then, as i am living in india and i am uneducated 10th pass, there is no where i will be able to make that python library popular. i feel this research is useful and world changing. that's why i want to share.",1749392910.0,computerscience,STEM
1l50nw5,"Why are compression algorithms based on Markov Chain better for compressing texts in human languages than Huffman Coding when it is not how real languages are behaving? It predicts that word-initial consonant pairs are strongly correlated with word-final ones, and they're not due to Law of Sonority.",,1749236799.0,computerscience,STEM
1l495kt,Mechanical Computer,"First mechanical computer I have seen in person.

",1749153974.0,computerscience,STEM
1l46wwn,Neuromorphic computing: the future of AI | LANL,,1749148600.0,computerscience,STEM
1l43dyz,What situation in the area of Networks would require you to use Bellman Fords algorithm instead of Djikstra’s because there are negative edge weights?,same as title. ,1749140339.0,computerscience,STEM
1l3uxtn,History - Modern replication of the first ´modern´ computers?,"There is the guy on yt, ho builds a shack in the jungle from nothing. It may help to understand basic principles.

Is there anything similar, that one builds a modern like computer WITHOUT using any commercially avaialable computer parts? ",1749116127.0,computerscience,STEM
1l3ts43,Learning DSA (Non programming),"Hi everyone, I know this is something discussed often, but hear me out. I want to learn Data Structures and Algorithms from scratch and not in the context of programming/leetcode/for the sake of interviews.

I really want to take my time and actually understand the algorithms and intuition behind them, see their proofs and a basic pseudocode.

Most online resources target the former approach and memorize patterns and focus on solving for interviews, I would really like to learn it more intuitively for getting into the research side of (traditional) computer science.

Any suggestions? ",1749111161.0,computerscience,STEM
1l3fu4x,What type of research is going on in PL,"Exploring potential research paths for grad studies. I have absolutely no PL knowledge/experience, just seems interesting to me.

What are some examples of research going on in PL and where’s a good place to get an intro to PL?",1749068102.0,computerscience,STEM
1l37ofe,Computer History,"I am in the process of creating a small organisation around teaching people about how to use a computer (starting from zero) which I havent incorperated yet but will either be a charity, a trading company or something inbetween. 

I am in the process of writing up a course and felt that it might be appropriate to begin with a short summary of the history of computers, which I begin with Alan Turing to avoid splitting hairs about ""what the first computer was"" and running into ever finer and finer definitions of a computer or suchlike. I aim to end the topic with teaching the very basics of computers - using a mouse and keyboard where I will go on from there.

Why talk about history when teaching people how to use a computer? 
My motivation for providing a brief history of computing is that it will subtley introduce some ideas that will be helpful to know when you are learning about how to *use* computers such as ""what is an operating system"". I am a fan of learning the etymology of words because I feel it helps me remember their meaning aswel as being generally interesting to read about (did you know Starbucks comes from a viking name for a river?), im hoping this will have a similar effect to its recipients. 

I want to start a discussion on this thread about the history of computers by asking you for anything interesting you know to do with important moments in the development of computers to help my research. I am only 19 so I have never known a world without mobile phones, internet, laser printing and a number of other miracles that I usually take for granted. I would be lying if this wasn't also about a personal curiosity. 
Anything you think is relevant here is welcome for discussion.

Thank you :)",1749049006.0,computerscience,STEM
1l3lk9g,Highschool extracurricular suggestions,I am a junior in highschool. Anybody know any good highschool extracurriculars for computer science majors,1749082982.0,computerscience,STEM
1l2o9xb,Computer Science exta-curriculars?,"Hi! Im just curious as to what extracurriculars programs there are for computer science/cyber security. Things like competitions, projects, certifications that i could complete over the summer

Im already working through the CISCO program, and i was wondering if there are any more as i believe theyre SO hard to find

Im 16 located in the UK, as I know some programs have an age or location requirements 

Thank you :)",1748986850.0,computerscience,STEM
1l2kwc5,inter vlan problem,"Hi there, trying to make a network on packet tracer, everything works but inter vlan communication, can't understand why. Here are screens :

https://preview.redd.it/5l7a5pk5kr4f1.png?width=1920&format=png&auto=webp&s=2eb7f2030bb3cb2ec051c28fa8f84f2e238d2822

https://preview.redd.it/z6mm0s26kr4f1.png?width=1920&format=png&auto=webp&s=971dc62ad44ba63bc48a36434ef31ea7988108a1

https://preview.redd.it/ihqu3wk6kr4f1.png?width=1920&format=png&auto=webp&s=8eab5ac3f03a0c5c196918ed40d0ffaf64fd16d4

if anyone has an idea, it would be super appreciated.

Thx and hf",1748978912.0,computerscience,STEM
1l1xgg5,It's Official: Physics Is Hard (by CS standards),,1748908008.0,computerscience,STEM
1l1lb1s,Any recommendations on learning and studying System architecture?,"Hey y'all, I am Wanting to dip my finger into learning System architecture and wanted to ask for some good resources 

Thank you",1748878864.0,computerscience,STEM
1l15rta,How actually did you guys learn reverse engineering?,"I am a highschooler, interested in the lowlevel stuffs, in order to learn and explore I tried reverse engineering to see what's inside it and how it's work.

But it seems kinda overwhelmed for a kid like me, I watched videos on yt and tried to explore dbg/disassembler tools yet still didnt understand what's going on. I didnt find any free course too.

Btw I know basic of computer architecture and how it works in general so I wanna start learning assembly too. Do u have any advice?

I know that I have to know engineering first before step into RE, but I'm open to know how you guys learned.",1748828222.0,computerscience,STEM
1l0mkz2,What is the digital version of this,,1748776871.0,computerscience,STEM
1l0jrkw,Is my paper conference worthy?,"Hi all,

I am a PhD student in theoretical computer science and have been working on a side paper for a bit. It deals with a variant of Hierholzer's algorithm for computing a Eulerian cycle in a Eulerian graph that does not require recursion or strict backtracking rules.

To the best of my knowledge, such a (minor) variant does not exist in the literature, so I would be interested in formalising it and providing a rigorous proof of correctness and complexity. However, since it would be a paper dedicated to a problem that is well studied, I do not know whether it would be conference worthy or deemed redundant.",1748765546.0,computerscience,STEM
1l072om,Couldn’t someone reverse a public key’s steps to decrypt?,"Hi! I have been trying to understand this for quite some time but it is so confusing…

  
When using a public key to encrypt a message, then why can’t an attacker just use that public key and reverse the exact same steps the public key says to take? 

I understand that, for example, mod is often used as if I give you X and W (in the public key), where W = X mod Y, then you multiply your message by W but you still don’t know Y. Which means that whoever knows X would be able to verify that it was truly them (the owner of the private key) due to the infinite number of possibilities but that is of no use in this context?

So then why can’t I just Divide by W? Or whatever the public key says to do?

Sorry if my question is simple but I was really curious and did not understand ChatGPT’s confusing responses!",1748724188.0,computerscience,STEM
1kzv34n,Can quorums be used to reject concurrent writes?,"I have a specific use case where certain operations on a replicated data type must never be performed concurrently. I'm wondering whether majority quorums can be leveraged to reject a write if it's concurrent with an already committed one.

  
My intuition is that this might be possible, since any two majority quorums intersect—meaning at least one process would observe both writes and could reject the later one. However, I'm concerned that achieving this behavior might actually require full consensus.",1748691725.0,computerscience,STEM
1kz524j,Paper Summary— Jailbreaking Large Language Models with Fewer Than Twenty-Five Targeted Bit-flips,,1748613852.0,computerscience,STEM
1kzanmy,Guidance for continue learning Computer Architecture,"Hello, Im a current final year CS undergrad and throughout my modules I was exposed to some ideas of Computer systems, OS, and Computer architecture and Compiler theory. I know the basics of many things but I would like to learn in depth, especially in CA. I was exposed the basics of pipelining, parallelism, multithreading, virutal memory and caches etc. The H&P book was refered in a module so naturally I would finish reading that. Apart from that where can I take the next steps towards to, with my current high level exposure to the ideas? 

Ive heard about the;

 nand2tetris, Computer Systems: A Programmer's Perspective, Tenebaum's ""Modern Operating Systems"", ""Code: The Hidden Language of Computer Hardware and Software"", Ben Eater""s Build an 8-bit computer from scratch etc.

Is there any resources here that would repeat what I already know? Or is there any recommended resource that I can take to continue? Or any order? I had a very unstructured learning of the theories and confused about the best place to continue.  

Would really appreciate any advice. Thanks in advance",1748627274.0,computerscience,STEM
1kzp2om,Shared Database Pattern in Microservices: When Rules Get Broken,"Everyone says ""never share databases between microservices."" But sometimes reality forces your hand - legacy migrations, tight deadlines, or performance requirements make shared databases necessary. The question isn't whether it's ideal (it's not), but how to do it safely when you have no choice.

The shared database pattern means multiple microservices accessing the same database instance. It's like multiple roommates sharing a kitchen - it can work, but requires strict rules and careful coordination.

Read More: [https://www.codetocrack.dev/blog-single.html?id=QeCPXTuW9OSOnWOXyLAY](https://www.codetocrack.dev/blog-single.html?id=QeCPXTuW9OSOnWOXyLAY)  
",1748667827.0,computerscience,STEM
1kye7k5,How much CS do I need to be familiar with to learn theoretical computer science?,"I'm really interested in mathematical logic, and its often involved in theoretical computer science. I know basically nothing about cs, but the little glimpses I have into theoretical cs make it seem really interesting. 
I don't want to study it professionally or academically, just for fun and maybe to see how it relates to math. I'm not worrying about applying anything personally or doing projects, I just want to learn about it.
I don't want to try jumping in without the right background knowledge and either be completely lost or misinterpret it. 
I would just be learning introductory stuff, not any specific subfield 
What basic computer science is necessary to kind of get the gist? Do I need to be familiar with a certain programming language? I don't much about computing at all, so I'm kind of going in blind.",1748533716.0,computerscience,STEM
1kzfdk0,LLMs replacing Google is just one search level deeper,"In the last couple of days, I've been thinking: Google does search in one way for us. chatGPT does that in a couple of ways, because it matching words and its linked information to it.",1748638873.0,computerscience,STEM
1kyh2oo,"Will quantum computers ever be available to everyday consumers, or will the always be exclusively used by companies, governments, and researchers?","I understand that they probably won't replace standard computers, but will there be some point in the future where computers with quantum technology will be offered to consumers as options alongside regular machines?",1748540499.0,computerscience,STEM
1kxeexs,New algorithm beats Dijkstra's time for shortest paths in directed graphs,,1748430896.0,computerscience,STEM
1kytk0r,"How hard would it be, theoretically, to get a search engine to be able to look through every YouTube video to get the best search results?","The example here is that typing something into the search bar for a certain video on YouTube didn't work. However, the thing I wanted to get out of the video came up in an unrelated video as a small part of it. More specifically, it was a video game boss fight with a specific attack used against the Final Boss, but whille typing it into YouTube didn't work, that exact sequence I wanted showed up as a very obscure part of another video, which would have satisfied my requests if the search engine knew to go through every YouTube video and bring that back as a possible result I'd be interested in. It would be easier if the search engine knew how to do this. 

So, my question is, how hard would it be, theoretically, to get a search engine to do this?",1748573393.0,computerscience,STEM
1kxjlng,"OSI Reference Model, Data Link Layer","The main task of the data link layer is to transform a raw transmission facility into a line that appears free of undetected transmission errors. (Computer Networks, A. Tanenbaum)

>appears free of undetected transmission errors.

How can we say anything is free of undetected errors ?   
What does 'undetected' even mean here ?",1748445095.0,computerscience,STEM
1kxc1iw,Opportunity in Security related to LLMs and conversational agents,"Hello everyone,

I recently discovered, thanks to my professor, a 3/6 months opportunity in the field of Security related to LLMs and conversational agents. As a first-year student, I know nothing about this topic, and I'd like to ask you if you could explain better this subject (currently I have to talk more to my professor, but I wanted to ask to you first)

Thank you in advance for your help!",1748421550.0,computerscience,STEM
1kwyc4t,"Does memoizing a function make it truly ""idempotent""?","If you cache the result of a function, or say, for instance, check to see if its already been run, and skipping running it a second time make a function truly idempotent?",1748378749.0,computerscience,STEM
1kxbpxi,Topological Sorting,"hi all, some personal research i have done on my own accord that can be explored further with regards to topological sorting are  
**Parallel Topological Sorting, Dynamic DAGs, Kahn's algorithm vs DFS sorting**.

Im hoping that the experts of this sub reddit can give me more insight in these areas or if there are any other areas of topological sorting i can explore further too! Thank you. Any insight/opinions will be greatly appreciated.",1748420184.0,computerscience,STEM
1kwg74n,What do you think is next gamechanging technology?,"Hi, Im just wondering what are your views on prospets of next gamechanging technology? What is lets say docker of 2012/15 of today? The only thing I can think of are softwares for automation in postquantum migration cause it will be required even if quantum computing wont mature.",1748326599.0,computerscience,STEM
1kvsyex,Resource on low level math optimisation,"Hello people. Im currently making a FEM matrix assembler. I want to have it work as efficiently as possible. Im currently programming it in python+numba but i might switch to Rust. 
I want to learn more about how to write code in a way that the compiler can optimise it as well as possible. I dont know if the programming language makes night and day differences but i feel like in general there should be information on heuristics that will guide me in writing my code so that it runs as fast as possible. I do understand that some compilers are more efficient at finding these optimisations than others. The type of stuff I’m referring to could be for example (pseudo code)

f(0,0) = a*b + c*d
f(1,0) = a*b - c*d

vs 

q1 = a*b
q2 = c*d
f(0,0) = q1+q2
f(1,0) = q1-q2

Does anyone know of videos/books/webpages to consult?
",1748262012.0,computerscience,STEM
1kvfo27,What exactly differentiates data structures?,"I've been thinking back on the DSA fundamentals recently while designing a new system, and i realised i don't really know where the line is drawn between different data structures.

It seems to be largely theoretical, as stacks, arrays, and queues are all udually implemented as arrays anyway, but what exactly is the discriminating quality of these if they can all be implemented at the same time?

Is it just the unique combination of a structure's operational time complexity (insert, remove, retrieve, etc) that gives it its own 'category', or something more?",1748214444.0,computerscience,STEM
1kv6nds,Alan Turing papers saved from shredder to be sold in Lichfield (UK) June 17,,1748190534.0,computerscience,STEM
1ku38kw,"One CS class, and now I'm addicted","I have taken a single college course on C++, and this is what it has brought me to. I saw a post about the birthday problem (if you don't know, it's a quick Google), and thought, ""I bet I can write a program to test this with a pretty large sample size"". Now here I am 1.5 hours later, with a program that tests the birthday problem with a range of group sizes from 1 to 100. It turns out it's true, at 23 people, there is a 50% chance of a shared birthday. ",1748061048.0,computerscience,STEM
1kuhxok,Anyone have tips for how I should study compilers?,"How can I go about learning compilers quickly and efficiently. Anyone have good links for -  but not limited to - learning about virtual machines, parsing machines, and abstract syntax trees? ",1748110091.0,computerscience,STEM
1kubw3s,What’s your process when you can’t trace how a system reaches its results?,"I regularly find myself in situations where I'm using a tool, library, or model that returns answers or outputs, but I can't see the process it follows to get there. If something doesn't seem quite right, strange, or surprising, it can be difficult to figure out what is going on behind the scenes and how to get to the bottom of the issue. If you have experienced a similar situation when you have had to work with something you don't feel comfortable fully inspecting what techniques do you take to either assess, understand, or simply build confidence in what it is doing? ",1748094076.0,computerscience,STEM
1ktfi35,C or C++ or some other lang,"I was thinking of learning a new lang, i want to pursue computer science eng, which is the best to learn for future

i know some basics of python and C,

I can allocate around an hour or two daily for atleast a year

i definitely want to go into game development or software development or some thing related to micro computers or microprocessors.",1747993570.0,computerscience,STEM
1ksd8sc,Why Are Recursive Functions Used?,"Why are recursive functions sometimes used? If you want to do something multiple times, wouldn't a ""while"" loop in C and it's equivalent in other languages be enough? I am not talking about nested data structures like linked lists where each node has data and a pointed to another node, but a function which calls itself.",1747872639.0,computerscience,STEM
1ks16n4,Best cs book you ever read?,"Hi all, what's the best computer science book you've ever read that truly helped you in your career or studies? I'd love to hear which book made a real difference for you and why.",1747842458.0,computerscience,STEM
1ks45ih,Best course for children?,"A friend's son (11 years old) has showed a big interest in coding and has made a little game using Scratch but he wants to get more into it. I suggested maybe python would be his best point to into. He looked at an online course but was sure it was a scam as they wanted £2k. 
Suggested a Udemy course for beginners or children but thinking actual parents might know more 🤣🤣.",1747849545.0,computerscience,STEM
1krv0xy,why is f(x) = |x^0.5| a function and why is f(x) = x^0.5 not a function?,"https://preview.redd.it/0166xw25a42f1.png?width=945&format=png&auto=webp&s=7efc7c8b6a462bede99c4cfa939b99e283710a90

",1747825406.0,computerscience,STEM
1kr6pyj,"Computing pioneer Alan Turing’s early work on “Can machines think?” published in a 1950 scholarly journal sold at the Swann Auction sale of April 22 for $10,000 or double the pre sale high estimate. Reported by RareBookHub.com","The catalog described the item as: Turing, Alan (1912-1954), Computing, Machinery, and Intelligence, published in Mind: a Quarterly Review of Psychology and Philosophy. Edinburgh: Thomas Nelson & Sons, Ltd., 1950, Vol. LIX, No. 236, October 1950.



First edition of Turing's essays posing the question, ""Can machines think?""; limp octavo-format, the complete journal in publisher's printed paper wrappers, with Turing's piece the first to appear in the journal, occupying pages 433-460.



The catalog comments: “With his interest in machine learning, Turing describes a three-person party game in the present essay that he calls the imitation game. Also known as the Turing test, its aim was to gauge a computer's capacity to interact intelligently through questions posed by a human. Passing the Turing test is achieved when the human questioner is convinced that they are conversing by text with another human. In 2025, many iterations of AI pass this test.” ",1747752448.0,computerscience,STEM
1krkzjx,"IF pairing Priority Queues are more efficient than Binary Priority Queues, why does the STL Use Binary?",C++,1747788234.0,computerscience,STEM
1kr4rhv,"Anyone here building research-based HFT/LFT projects? Let’s talk C++, models, frameworks","I’ve been learning and experimenting with both C++ and Python — C++ mainly for understanding how low-latency systems are actually structured, like:

Multi-threaded order matching engines

Event-driven trade simulators

Low-latency queue processing using lock-free data structures

Custom backtest engines using C++ STL + maybe Boost/Asio for async simulation

Trying to design modular architecture for strategy plug-ins


I’m using Python for faster prototyping of:

Signal generation (momentum, mean-reversion, basic stat arb models)

Feature engineering for alpha

Plotting and analytics (matplotlib, seaborn)

Backtesting on tick or bar data (using backtesting.py, zipline, etc.)


Recently started reading papers from arXiv and SSRN about market microstructure, limit order book modeling, and execution strategies like TWAP/VWAP and iceberg orders. It’s mind-blowing how much quant theory and system design blend in this space.

So I wanted to ask:

Anyone else working on HFT/LFT projects with a research-ish angle?

Any open-source or collaborative frameworks/projects you’re building or know of?

How do you guys structure your backtesting frameworks or data pipelines? Especially if you're also trying to use C++ for speed?

How are you generating or accessing tick-level or millisecond-resolution data for testing?


I know I’m just starting out, but I’m serious about learning and contributing neven if it’s just writing test modules, documentation, or experimenting with new ideas. If any of you are building something in this domain, even if it’s half-baked, I’d love to hear about it.

Let’s connect and maybe even collab on something that blends code + math + markets.
Peace.",1747747388.0,computerscience,STEM
1kr9aui,Can you teach me about Mealie and Moore Machines?,"Can you teach Mealie and Moore's machines. I have Theory of Computation as a subject. I do understand Finite State Transducers and how they are defined as a five tuple formally. (As given in Michael Sipser's Theory of Computation) But I don't get, the Moore's machines idea that the output is associated with the state, unlike in Mealy machines where each transition has an output symbol attached. Also, I read in Quora that Mealy and Moore Machines have 6 tuples in their formal definitions, where one is the output transition. 

Thanks and regards. ",1747758640.0,computerscience,STEM
1kr6g5h,How good is your focus?,"I’ve been self studying computer architecture and programming. I’ve been spending a lot of time reading through very dense textbooks and I always struggle to maintain focus for long durations of time. I’ve gotten to the point where I track it even, and the absolute maximum amount of time I can maintain a deep concentrated state is precisely 45 mins. I’ve been trying to up this to an hour or so but it doesn’t seem to budge, it’s like 45 mins seems to be my max focus limit. I know this is normal, but I’m wondering if anyone here has ever felt the same? For how long can you stay engaged and focus when learning something new and challenging?",1747751778.0,computerscience,STEM
1kpu092,A computer scientist's perspective on vibe coding:,,1747601148.0,computerscience,STEM
1kr2yfa,Which CS subfields offer strong theoretical foundations with real-world impact for undergraduates?,"I'm exploring which areas of computer science are grounded in strong theory but also lead to impactful applications. Fields like cryptography, machine learning theory, and programming language design come to mind, but I'm curious what others think.

Which CS subfields do you believe offer the most potential for undergraduates to explore rigorous theory while contributing to meaningful, long-term projects?

Looking forward to hearing your insights.",1747742027.0,computerscience,STEM
1kqvtg3,"Throttles, frontend or backend responsibility?","I assumed it was front end, but that seems like it creates an opportunity for abuse by the user. However, I thought the purpose of the throttle was to reduce the amount of api calls to the server, hence having it on the backend just stops a call before it does anything, but doesn't actually reduce the number of calls. ",1747713765.0,computerscience,STEM
1kqih1q,"Assembly IDE in the Web: Learn MIPS, RISC-V, M68K, X86 assembly","Hello everyone!  
During my first CS year i struggled with systems programming (M68K and MIPS assembly) because the simulators/editors that were suggested to us were outdated and lacked many useful features, especially when getting into recursion.

That's why i made [https://asm-editor.specy.app/](https://asm-editor.specy.app/), a Web IDE/simulator for MIPS, RISC-V, M68K, X86 (and more in the future) Assembly languages.

It's open source at [https://github.com/Specy/asm-editor](https://github.com/Specy/asm-editor), Here is a [recursive fibonacci function in MIPS](https://shorturl.at/CygXX) to show the different features of the IDE. 

https://preview.redd.it/10olx6fm1s1f1.png?width=1918&format=png&auto=webp&s=bb14278cc5539752a175dcb2583ee430bcead633

Some of the most useful features are:

* instruction undo and step 
* breakpoints
* function stack tracing and stack frame view. 
* history viewer (shows the side effects of each instruction)
* I/O and memory viewer (both number and text)
* number conversions for registers and memory 
* testcases (useful for professors making exercises)
* auto completion and inline errors, etc...

There is also a feature to embed the editor inside other websites, so if you are a professor making courses, or want to use the editor inside your own website, you can!

Last thing, i just finished implementing a feature that allows interactive courses to be created. If you are experienced in assembly languges and want to help other students, come over on the [github repo](https://github.com/Specy/asm-editor/issues/27) to contribute!",1747677665.0,computerscience,STEM
1kq8lge,"When is a deck of cards ""truly shuffled""?","Hey! I wrote this article recently about mixing times for markov chains using deck shuffling as the main example. It has some visualizations and explains the concept of ""coupling"" in what-I-hope a more intuitive way than typical textbooks.

Looking for any feedback to improve my writing style + visualization aspects in these sort of semi-academic settings.",1747651593.0,computerscience,STEM
1kq0276,Built simple http server in c,"I've built a simple HTTP server in C 
It can handle multiple requests, serve basic HTML and image files, and log what's happening.
I learned a lot about how servers actually work behind the scenes.

Github repo : https://github.com/sandeepsandy62/Httpserver",1747618806.0,computerscience,STEM
1kp5o55,Is it worth pursuing an alternative to SIMT using CPU-side DAG scheduling to reduce branch divergence?,"Hi everyone,
This is my first time posting here, and I’m genuinely excited to join the community.

I’m an 18-year-old self-taught enthusiast deeply interested in computer architecture and execution models. Lately, I’ve been experimenting with an alternative GPU-inspired compute model — but instead of following traditional SIMT, I’m exploring a DAG-based task scheduling system that attempts to handle branch divergence more gracefully.

The core idea is this: instead of locking threads into a fixed warp-wide control flow, I decompose complex compute kernels (like ray intersection logic) into smaller tasks with explicit dependencies. These tasks are then scheduled via a DAG, somewhat similar to how out-of-order CPUs resolve instruction dependencies, but on a thread/task level. There's no speculative execution or branch prediction; the model simply avoids divergence by isolating independent paths early on.

All of this is currently simulated entirely on the CPU, so there's no true parallel hardware involved. But I've tried to keep the execution model consistent with GPU-like constraints — warp-style groupings, shared scheduling, etc. In early tests (on raytracing workloads), this approach actually outperformed my baseline SIMT-style simulation. I even did a bit of statistical analysis, and the p-value was somewhere around 0.0005 or 0.005 — so it wasn't just noise.

Also, one interesting result from my experiments:
When I lock the thread count using constexpr at compile time, I get around 73–75% faster execution with my DAG-based compute model compared to my SIMT-style baseline.

However, when I retrieve the thread count dynamically using argc/argv (so the thread count is decided at runtime), the performance boost drops to just 3–5%.

I assume this is because the compiler can aggressively optimize when the thread count is known at compile time, possibly unrolling or pre-distributing tasks more efficiently. But when it’s dynamic, the runtime cost of thread setup and task distribution increases, and optimizations are limited.

That said, the complexity is growing. Task decomposition, dependency tracking, and memory overhead are becoming a serious concern. So, I’m at a crossroads:
Should I continue pursuing this as a legitimate alternative model, or is it just an overengineered idea that fundamentally conflicts with what makes SIMT efficient in practice?

So as title goes, should I go behind of this idea? I’d love to hear your thoughts, even if critical. I’m very open to feedback, suggestions, or just discussion in general. Thanks for reading!",1747522699.0,computerscience,STEM
1kpt7x8,I need help understanding avl trees for my data structures final tomorrow,"I have been trying to study avl trees for my final and I keep running into to conflicting height calculations. I am going to provide a few pictures of what my professor is doing because I can’t understand what she is doing. I understand it that the balance factor is height of left subtree - height of right subtree. And the height of a subtree is the number of edges to a leaf node. I’m pretty sure I understand how rotations work but whenever I try to practice the balance factor is always off and I don’t know which is which because my professor seems like she is doing 2 different height calculations. 

Also if anyone has any resources to practice avl trees and their rotations 

Thank you for any and all h! ",1747599106.0,computerscience,STEM
1koinxc,Book recommendations?,"Hello everyone! I was hoping for some help with book recommendations about chips. I’m currently reading The Thinking Machine by Stephen Witt, and planning to read Chip Wars along with a few other books about the history and impact of computer chips. I’m super interested in this topic and looking for a more technical book to explain the ins and outs of computer hardware/architecture rather than a more journalistic approach on the topic, which is what I’ve been reading.  

Thank you!!",1747449730.0,computerscience,STEM
1knwhiu,"Machine learning used to be cool, no?","Remember deepdream, aidungeon 1, those reinforcement learning and evolutionary algorithm showcases on youtube? Was it all leading to this nightmare? Is actually fun machine learning research still happening, beyond applications of shoehorning text prediction and on-demand audiovisual slop into all aspects of human activity? Is it too late to put the virtual idiots we've created back into their respective genie bottles?",1747387269.0,computerscience,STEM
1koesk6,New computer shortcuts cut method (idea),"Please correct if I am wrong. I am not an expert.

From my understanding computer shortcuts go through specific directory for example: \C:\folder A\folder B\ “the file”
 It goes through each folder in that order and find the targeted file with its name.
But the problem with this method is that if you change the location(directory) of the file the shortcut will not be able to find it because it is looking through the old location.

My idea is to have for every folder and files specific ID that will not change. That specific ID will be linked to the file current directory. Now the shortcut does not go through the directory immediately, but instead goes to the file/folder ID that will be linked to the current directory. 
Now if you move the folder/file the ID will stay the same, but the directory associated with that ID will change. Because the shortcut looks for the ID it will not be affected by the directory change.",1747437559.0,computerscience,STEM
1ko1jfp,is it possible to implement a quantum/photonics chip in a video game circuit for the sole purpose of ray tracing?,Light is inherently a quantum phenomenon that we're attempting to simulate on non-quantum circuits. wouldn't it be more efficient to simulate in its more natural quantum environment?,1747403917.0,computerscience,STEM
1knmokt,How to decompose 1NF to 2NF and then 2NF to 3NF?,"My teacher told me that to decompose from 1NF to 2NF:

1. Find all of the candidate keys (CKs).
2. Identify the partial functional dependencies (PFDs).
3. Move the determinant and dependent of each PFD into a separate table.
4. From the original relation, remove the dependent of each PFD, and you will get 2NF.

For 2NF to 3NF, you follow the same steps for transitive functional dependencies (TFDs). However, there is an issue:

Consider the following functional dependencies (FDs):

* AB → C
* B → D
* D → E

Here, B → D is a partial functional dependency (PFD). Following the steps described by my teacher, we get:

* R1(B, D)
* R2(A, B, C, E)

But now, we have lost the FD D → E. What is the correct way to handle this?

I checked on YouTube and found many methods. One of them involves the following:

1. Find all of the candidate keys (CKs).
2. Identify the PFDs.
3. Take the closure of the determinant of each PFD and move those attributes into a separate table.
4. From the original relation, remove the attributes obtained from the closure (except for the trivial dependencies).

The same steps are to be followed for TFDs when decomposing from 2NF to 3NF.

Is this method more correct? Any help would be highly appreciated.",1747352011.0,computerscience,STEM
1kn159w,Most underground and unknown stuff,"Which kind of knowledge you think is really underground and interesting, but usually nobody looks up?",1747289447.0,computerscience,STEM
1kmiqoe,Is this an accurate diagram of a CPU block?,"I am doing a university module of computer systems and security. It is a Time Constraint Assessment so I have little idea of what the questions will be, but I am of the assumption that it will be things like ""explain the function of X"". In one of the online supplementary lessons there is a brief description of a CPU and a crude diagram with modals to see more about each component, but looking at diagrams from other sources I am getting conflicting messages.

  
From what I've gather from the various diagrams, this is what I came to. I haven't added any data bus and control bus arrows yet, but for the most part they're just 2 way arrows between each of the components which I don't really get because I was under the impression the Fetch-Decode-Execute was a cycle and cycles usually go round linearly.

  
Would you say this is an accurate representation of a CPU block? If not, what specifically could I add/change/remove to improve it?",1747238131.0,computerscience,STEM
1kn3nm3,Asynchronous Design Resources,,1747300025.0,computerscience,STEM
1kmpnbd,Designing an 8-bit CPU: How to load constants?,"I have been following Sebastian Lague's videos on YouTube and have started to make my own CPU in his Digital Logic Sim. Currently it is single cycle and I have registers A and B, a program counter, a basic ALU and ROM for the program.

My goal is to run a program that outputs the Fibonacci sequence. I have a very basic control unit which has output bits for:

* Write to A
* Write to B
* Output A
* Output B
* Output ALU

With this I have made an ADD instruction which adds A and B and writes the output to A. 

I now need an instruction to load a constant into either A/B. I've looked online but am quite confused how to implement this. I've seen examples which have the immediate constant, e.g.: XXXXAAAA, where X is the opcode and A is the constant (ideally I want to learn how to load 8 bit numbers, so this won't work for me).

I've seen other examples where it uses microcode and 2 bytes, e.g.: the first byte is the instruction to load a constant, and the second is the actual constant (which would allow for 8 bits).

What would be the best way to implement the microcode? Would this be possible for a single cycle CPU? Do I need an instruction register? I also don't want the CPU to execute the data, so how would I correctly increment the program counter? Just increment it twice?",1747254581.0,computerscience,STEM
1kmalt7,How screwed am I if I don’t know discrete math,I did a discrete math course and it was an awful time. It was online and the professor just read from the textbook. Asking question and taking note did not help.I did not drop it because it was my first time as a student in higher level education so I was scared but now I regret it. In the end they rounded up grades. It has been a while and I have forgoten what little I had learned. I know that it is used in artificial intelligent classes and others. I have the option to do the course again in different environment. But I want to know what would happen if I take these classes with no information in discrete math.,1747213668.0,computerscience,STEM
1kmrlty,Master thesis effective time management,"Hi, 
I want to get your advice, follow Redditors, about how to manage well quality time working on my thesis.

I am in the reading stage and my thesis is on the theoretical side. I've been logging my work this first 2 weeks. I've been spending around 8 hours of total work per day on the thesis however I notice that I can only have 4h30mins average active focus. The rest of the time I just lose focus easily, I get sick of reading the same proof for an entire day or I start taking more breaks, especially on the afternoons. 

I am trying to be more effective, your advise are welcome :)",1747259410.0,computerscience,STEM
1kmo8rw,Tell us what you think about our computational Biology preprint,,1747251172.0,computerscience,STEM
1km4dkp,Calculating checksum collisions appropriately,"Hi all, 

I stumbled across the following problem.  
  
I need to uniquely identify data in my database using an id, however I am constrained by the length. As a result, along with various other verification tools, I am sending a checksum to confirm that the Id has not changed. I am worried about collisions.

Unlike the birthday problem, I am not necessarily worried about a duplicate overall (since I am checking more than just the checksum, and the checksum is the last thing I check). Instead I think I am worried about:

How many pairs of checksums do I need to see before there is a greater than 50% of a collision? What if instead of pairs its a group with G elements and M the modulo used for my checksum?

Here is the calculation I am thinking of:

The probability of no collision is (M-1)/M \* (M-2)/M \* ... \* (M-G+1)/M  = (M-1)!/((M-G)! \* M\^(G-1)) = P

The probability of a collision in a group is 1 - P.

To solve for the number of groups required for the probability of a collision to be equal to 0.5 is :

0.5 = 1 - (P)\^X where X is the number of groups.

Then as a follow up, I realize that I am assuming that the distribution of Ids I am checking is randomly distributed. I know for a fact that the Ids I am recieving are not random. This is leading me to consider different checksum algorithms.

The one that I am familiar with is a polynomial rolling hash that uses a large prime number for its modulo. However, when doing the calculations I am questioning whether the polynomial rolling part does anything. Further, since this Ids are generated using an algorithm (dont know which algorithm for the record), I have reason to believe that they will be sequential and I am not worried about security or checking if a bit is wrong. This leads me to two additional questions:

1) Does the polynomial rolling part actually make it worse? If my data is sequential and I take it to a prime number mod, won't I exhaust every entry until I hit the modulo? In the other scenario, I may get an unlucky mapping and hit a collision sooner?

2) In this case, what does polynomial rolling even provide? Is that more for the purpose of hashing where security concerns are necessary, or a case where we want to check if bits may have been mistyped?

I apologize if this is a basic question, I do not know much about cryptography (maybe this should have went in that sub instead), and none of the basic literature I could find via google search had a satisfactory answer.",1747189866.0,computerscience,STEM
1kl22sy,Basic question about parallel repetition in IP protocol,"The book Sanjeev Arora and Barak defines class IP (\[interactive protocol\]\[1\]) by making the verifier have private coins. Before proceeding to public coin proofs and showing they are the ""same,"" the book mentions the following:



\> The probabilities of correctly classifying an input can be made arbitrarily close to 1 by using

 the same boosting technique we used for BPP: to replace $2/3$ by $1−e\^{−m}$,

 sequentially repeat the protocol m times and take the majority answer. In fact, using a more

 complicated proof, it can be shown that we can decrease the probability without increasing the

 number of rounds using parallel repetition (i.e., the prover and verifier will run $m$ executions

 of the protocol in parallel). 



Why does the naive idea of simply having the verfier and prover exchange an array of polynomial many messages  (different  copies) in each round not work? This doesn't increase the rounds. Assuming that for each copy, the verifier uses independent random coins.







  \[1\]: [https://en.wikipedia.org/wiki/Interactive\_proof\_system](https://en.wikipedia.org/wiki/Interactive_proof_system)",1747079364.0,computerscience,STEM
1kk8102,What books would you recommend as an introduction to computer science?,"I'm not looking for a book on coding languages, rather I'm looking to focus on the fundamentals. I've been recommended, Code: the hidden language of computer hardware and software 2nd edition. What do you all think?",1746988850.0,computerscience,STEM
1kkbgi5,How to train a model,"Hey guys, I'm trying to train a model here, but I don't exactly know where to start.

I know that you need data to train a model, but there are different forms of data, and some work better than others for some reason. (csv, json, text, etc...)

As of right now, I believe I have an abundance of data that I've backed up from a database, but the issue is that the data is still in the form of SQL statements and queries.

Where should I start and what steps do I take next?

Thanks!",1746997786.0,computerscience,STEM
1kjffzv,Flip Flops and Stochastic Processes,"

Flip flops are components within computer architecture which can store and manipulate data. The output of the flip flop is dependent on past events. So, could you model flip flops as a stochastic process like a Markov chain?  
",1746897793.0,computerscience,STEM
1kk3d50,What if a float number has an exponent greater than 23 ?,"Because Mantissa is 23 bits , I think it is meaningless to use a magnitude greater than 23, in which case you will have to skip lots of integer numbers which can only be represented with more than 23 bits . The numerical values beyond power 23 would be like , uhhhh , quantum .They are not even continuous in integer . May I ask what is the use case of a float with exponent greater than 23 ? I see the exponent can be up to 127, where are the magnitude between 23\~127 to be used ? ",1746976580.0,computerscience,STEM
1kikar2,Hashing isn’t just for lookups: How randomness helps estimate the size of huge sets,"Link to blog: [https://www.sidhantbansal.com/2025/Hashing-when-you-want-chaos/](https://www.sidhantbansal.com/2025/Hashing-when-you-want-chaos/)

Looking for feedback on this article I wrote recently.",1746802024.0,computerscience,STEM
1kje60g,how to be Updated on CS,I see that all you guys are highly updated on coding and stuffs how do you guys maintain this .,1746894329.0,computerscience,STEM
1kj3m71,A question about fundamental structure of algorithms,"I want to ask a question about algorithms, but it requires a bit of set up. 

# The basic truth 

Any minimally interesting algorithm has the following properties:
1. It solves a non-trivial problem via repeating some **key computation** which does most of the work. Any interesting algorithm *has to* exploit a repeating structure of a problem or its solution space. Otherwise it just solves the problem ""in one step"" (not literally, but conceptually) or executes a memorized solution. 
2. The key computation ""aims"" at something significantly simpler than the full solution to the problem. We could solve the problem in one step if we could aim directly at the solution. 
3. Understanding the key computation might be much easier than understanding the full justification of the algorithm (i.e. the proof that the key computation solves the problem), yet understanding the key computation is all you need to understand what the algorithm does. Also, if the problem the algorithm solves is big enough, you need much less computation to notice that an algorithm repeats the key computation (compared to the amount of computation you need to notice that the algorithm solves the problem).  
 
Those properties are pretty trivial. Let's call them ""the basic truth"". 

Just in case, here are some examples of how the basic truth relates to specific algorithms:
* [Bubble sort](https://en.wikipedia.org/wiki/Bubble_sort). The key computation is running a ""babble"" through the list. It just pushes the largest element to the end (that's significantly simpler than sorting the entire list). You can understand the ""babble"" gimmick much earlier than the list gets sorted. 
* [Simulated annealing](https://en.wikipedia.org/wiki/Simulated_annealing#Overview). The key computation is jumping from point to point based on ""acceptance probabilities"". It just aims to choose a better point than the current one, with some probability (much easier goal than finding the global optimum). You can understand the gimmick much earlier than the global optimum approximation is found.  
* Any greedy algorithm is an obvious example.
* Consider the algorithm which finds the optimal move in a chess position via brute-force search. The key computation is expanding the game tree and [doing backward induction](https://en.wikipedia.org/wiki/Game_tree#Solving_game_trees) (both things are significantly simpler than finding the full solution). You can understand what the algorithm is doing much earlier than it finds the full solution.
* Consider [chess engines](https://en.wikipedia.org/wiki/Chess_engine). They try to approximate optimal play. But the key computation aims for something much simpler: ""win material immediately"", ""increase the mobility of your pieces immediately"", ""defend against immediate threats"", ""protect your king immediately"", etc. Evaluation functions are based on those much simpler goals. You can understand if something is a chess engine long before it checkmates you even once.     

[Pseudorandom number generators](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) are counterexamples. You can't understand what a PRNG is doing before you see the output and verify that it's indeed pseudorandom. However, ""generate pseudorandom numbers"" is a very special kind of problem. 

There are also tricky cases when an algorithm (e.g. evolution or gradient descent) creates another algorithm.  


*** 

# The non-basic truth 

On closer inspection, the basic truth is not that basic:
* How would we formalize it rigorously?  
* To which levels of analysis does the ""truth"" apply to? Computational? Representational? Physical? (see David Marr)  
* The core of an algorithm can be understood ""much earlier than it solves the problem"", but is it true *in practice*, when you struggle with interpreting the algorithm? In what sense is it true/false in practice?  
* As I said, pseudorandom number generators are a caveat to the ""truth"".  
* I didn't discuss it earlier, but some algorithms have multiple ""key computations"". How do we formalize that the number of key computations should be very small? Small relative to what? 
* In the example with chess engines, the key computation might be done only implicitly/""counterfactually"" (if two strong engines are playing against each other, you might not see that they pursue simple goals unless you force one of the engines to make a very suboptimal move).  
   
**What research along those lines exists, if any?** That's my question.  

I only find the concept of [loop invariants](https://en.wikipedia.org/wiki/Loop_invariant), but it seems much less broad and isn't about proving properties of algorithms in general. Though I'm not sure about any of that.

**Why researching this matters?** The ""key computation"" is the most repeated and the most understandable and the most important part of an algorithm, so if you want to understand a hard to interpret algorithm, you probably need to identify its key computation. This matters for [explainability/interpretability](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence).     ",1746857899.0,computerscience,STEM
1khtocr,"I've been watching a video explaining the simplex method for linear programming. I got to this screen, and I have a question","First, I watched the video several times to make sure that the lecturer in the video didn't explain the points that I didn't understand.

What exactly is Cb? Is that something I'm supposed to know before I dive into the simplex method? And why are all the values 0? And when he determined the pivot row, he replaced the third Cb value (which was 0) with -3. Why? 

It may look like a dumb point to not understand, but I'm really bad at solving linear programming problems.

I humbly ask you to explain it to me like you're explaining it to a 8 yo kid.

And have a nice day!

",1746719938.0,computerscience,STEM
1khj325,Is this Linear Programming Formulation of Graph Isomorphism Problem correct?,"I was working on the TSP as a hobby and I noticed that we can express the graph isomorphism problem (GIP) as a linear programming problem but I'm not sure if this is correct because GIP is a complicated problem. You can find more details of the properties I use in this [working paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5209988).  
For those who want to try the model, in this [link](https://financioneroncios.wordpress.com/2025/05/08/a-new-linear-programming-formulation-of-the-graph-isomorphism-problem/) I created an example using Python and CVXPY. I recommend using a commercial solver like MOSEK, as this model has a number of variables and constraints proportional to n\^{4}.",1746684447.0,computerscience,STEM
1khsyh7,Whats the easiest way to understand/memorize the multiple access protocols and what each one is known for,"Im stuck on the 3 protocols random access, controll access and channelization, ive memorized their protocols but i cant seem to understand what each is really for, like for example when im asked “which one is to prevent errors” or “which one uses code, frequency or bandwidth” it doesnt make sense to me cause dont they all use it aand have their own methods of preventing errors? ",1746718143.0,computerscience,STEM
1kgvi2v,My Confusion about Addresses,"I'm trying to better understand how variables and memory addresses work in C/C++. For example, when I declare `int a = 10;`, I know that `a` is stored somewhere in memory and has an address, like `0x00601234`. But I'm confused about what exactly is stored in RAM. Does RAM store both the address and the value? Or just the value? Since the address itself looks like a 4-byte number, I started wondering — is the address stored alongside the value? Or is the address just the position in memory, not actually stored anywhere? And when I use `&a`, how does that address get generated or retrieved if it's not saved in RAM? I’m also aware of virtual vs physical addresses and how page tables map between them, but I’m not sure how that affects this specific point about where and how addresses are stored. Can someone clarify what exactly is stored in memory when you declare a variable, and how the address works under the hood?",1746619052.0,computerscience,STEM
1kgw29z,How to carry over DSA Skills from one language to another?,"I'm a student and fairly new to the entire DSA thing. I've been using c++ to solve basic problems. 

Recently i discovered that python offers simple ways to carry out things that would take me hours to code in c++. 

Do i just make the switch over to python or stick to c++?",1746620777.0,computerscience,STEM
1kgtiir,Gray-Hamming Distance Fractal,"[Gray-Hamming Distance Fractal 1..10 bits GIF](https://i.redd.it/paylde27qbze1.gif)

First of all, I don't know whether this is really a fractal, but it looks pretty cool.  
Here is Google Colab link where you can play with it: [Gray-Hamming Distance Fractal.ipynb](https://colab.research.google.com/drive/1qCz_4BlAmaPLS4tSU9zjPvenaUzUd9L5?usp=sharing)

The recipe:

1. **Start with Integers:** Take a range of integers, say 0 to 255 (which can be represented by 8 bits).
2. **Gray Code:** Convert each integer into its corresponding Gray code bit pattern.
3. **Pairwise Comparison:** For every pair of Gray code bit patterns`(j, k)` calculate the **Hamming distance between these two Gray code patterns**
4. **Similarity Value:** Convert this Hamming distance `(HD)` into a similarity value ranging from -1 to 1 using the formula: `Similarity = 1 - (2 * HD / D)`where `D` is the number of bits (e.g. 8 bits)
   * This formula is equivalent to the cosine similarity of specific vectors. If we construct a D-dimensional vector for each Gray code pattern by summing `D` orthonormal basis vectors, where each basis vector is weighted by `+1` or `-1` according to the corresponding bit in the Gray code pattern, and then normalize the resulting sum vector to unit length (by dividing by `sqrt(D)`), the dot product (and thus cosine similarity) of any two such normalized vectors is precisely `1 - (2 * HD / D)`
5. **Visualize:** Create a matrix where the pixel at `(j,k)` is colored based on this `Similarity`value.

The resulting image displays a distinct fractal pattern with branching, self-similar structures.

[Gray-Hamming Distance Fractal 8bits](https://preview.redd.it/kr6wsx29xbze1.png?width=779&format=png&auto=webp&s=8c0b4c455a476a0fbab889cad3b2581c66df0247)

I'm curious if this specific construction relates to known fractals.",1746611856.0,computerscience,STEM
1kh0548,What is TDD and BDD? Which is better?,"I wrote this short article about TDD vs BDD because I couldn't find a concise one. It contains code examples in every common dev language. Maybe it helps one of you :-)
Here is the repo: https://github.com/LukasNiessen/tdd-bdd-explained

# TDD and BDD Explained

_TDD = Test-Driven Development_  
_BDD = Behavior-Driven Development_

## Behavior-Driven Development

BDD is all about the following mindset: **Do not test code. Test behavior.**

So it's a shift of the testing mindset. This is why in BDD, we also introduced new terms:

- **Test suites** become **specifications**,
- **Test cases** become **scenarios**,
- We don't **test code**, we **verify behavior**.

Let's make this clear by an example.

## Java Example

If you are not familiar with Java, look in the repo files for other languages (I've added: Java, Python, JavaScript, C#, Ruby, Go).

```java
public class UsernameValidator {

    public boolean isValid(String username) {
        if (isTooShort(username)) {
            return false;
        }
        if (isTooLong(username)) {
            return false;
        }
        if (containsIllegalChars(username)) {
            return false;
        }
        return true;
    }

    boolean isTooShort(String username) {
        return username.length() < 3;
    }

    boolean isTooLong(String username) {
        return username.length() > 20;
    }

    // allows only alphanumeric and underscores
    boolean containsIllegalChars(String username) {
        return !username.matches(""^[a-zA-Z0-9_]+$"");
    }
}
```

UsernameValidator checks if a username is valid (3-20 characters, alphanumeric and \_). It returns true if all checks pass, else false.

How to test this? Well, if we test if the code does what it does, it might look like this:

```java
@Test
public void testIsValidUsername() {
    // create spy / mock
    UsernameValidator validator = spy(new UsernameValidator());

    String username = ""User@123"";
    boolean result = validator.isValidUsername(username);

    // Check if all methods were called with the right input
    verify(validator).isTooShort(username);
    verify(validator).isTooLong(username);
    verify(validator).containsIllegalCharacters(username);

    // Now check if they return the correct thing
    assertFalse(validator.isTooShort(username));
    assertFalse(validator.isTooLong(username));
    assertTrue(validator.containsIllegalCharacters(username));
}
```

This is not great. What if we change the logic inside isValidUsername? Let's say we decide to replace `isTooShort()` and `isTooLong()` by a new method `isLengthAllowed()`?

**The test would break**. Because it almost mirros the implementation. Not good. The test is now tightly coupled to the implementation.

In BDD, we just verify the behavior. So, in this case, we just check if we get the wanted outcome:

```java
@Test
void shouldAcceptValidUsernames() {
    // Examples of valid usernames
    assertTrue(validator.isValidUsername(""abc""));
    assertTrue(validator.isValidUsername(""user123""));
    ...
}

@Test
void shouldRejectTooShortUsernames() {
    // Examples of too short usernames
    assertFalse(validator.isValidUsername(""""));
    assertFalse(validator.isValidUsername(""ab""));
    ...
}

@Test
void shouldRejectTooLongUsernames() {
    // Examples of too long usernames
    assertFalse(validator.isValidUsername(""abcdefghijklmnopqrstuvwxyz""));
    ...
}

@Test
void shouldRejectUsernamesWithIllegalChars() {
    // Examples of usernames with illegal chars
    assertFalse(validator.isValidUsername(""user@name""));
    assertFalse(validator.isValidUsername(""special$chars""));
    ...
}
```

Much better. If you change the implementation, the tests will not break. They will work as long as the method works.

Implementation is irrelevant, we only specified our wanted behavior. This is why, in BDD, we don't call it a _test suite_ but we call it a _specification_.

Of course this example is very simplified and doesn't cover all aspects of BDD but it clearly illustrates the core of BDD: **testing code vs verifying behavior**.

## Is it about tools?

Many people think BDD is something written in Gherkin syntax with tools like Cucumber or SpecFlow:

```gherkin
Feature: User login
  Scenario: Successful login
    Given a user with valid credentials
    When the user submits login information
    Then they should be authenticated and redirected to the dashboard
```

While these tools are great and definitely help to implement BDD, it's not limited to them. BDD is much broader. BDD is about behavior, not about tools. You can use BDD with these tools, but also with other tools. Or without tools at all.

## More on BDD

https://www.youtube.com/watch?v=Bq_oz7nCNUA (by Dave Farley)  
https://www.thoughtworks.com/en-de/insights/decoder/b/behavior-driven-development (Thoughtworks)

---

## Test-Driven Development

TDD simply means: Write tests first! Even before writing the any code.

So we write a test for something that was not yet implemented. And yes, of course that test will fail. This may sound odd at first but TDD follows a simple, iterative cycle known as Red-Green-Refactor:

- **Red**: Write a failing test that describes the desired functionality.
- **Green**: Write the minimal code needed to make the test pass.
- **Refactor**: Improve the code (and tests, if needed) while keeping all tests passing, ensuring the design stays clean.

This cycle ensures that every piece of code is justified by a test, reducing bugs and improving confidence in changes.

## Three Laws of TDD

Robert C. Martin (Uncle Bob) formalized TDD with three key rules:

- You are not allowed to write any production code unless it is to make a failing unit test pass.
- You are not allowed to write any more of a unit test than is sufficient to fail; and compilation failures are failures.
- You are not allowed to write any more production code than is sufficient to pass the currently failing unit test.

## TDD in Action

For a practical example, check out this video of Uncle Bob, where he is coding live, using TDD: https://www.youtube.com/watch?v=rdLO7pSVrMY

It takes time and practice to ""master TDD"".

## Combine them (TDD + BDD)!

TDD and BDD complement each other. It's best to use both.

TDD ensures your code is correct by driving development through failing tests and the Red-Green-Refactor cycle. BDD ensures your tests focus on what the system should do, not how it does it, by emphasizing behavior over implementation.

Write TDD-style tests to drive small, incremental changes (Red-Green-Refactor). Structure those tests with a BDD mindset, specifying behavior in clear, outcome-focused scenarios.
This approach yields code that is:

- Correct: TDD ensures it works through rigorous testing.
- Maintainable: BDD's focus on behavior keeps tests resilient to implementation changes.
- Well-designed: The discipline of writing tests first encourages modularity, loose coupling, and clear separation of concerns.

## Another Example of BDD

Lastly another example.

Non-BDD:

```java
@Test
public void testHandleMessage() {
    Publisher publisher = new Publisher();
    List<BuilderList> builderLists = publisher.getBuilderLists();
    List<Log> logs = publisher.getLogs();

    Message message = new Message(""test"");
    publisher.handleMessage(message);

    // Verify build was created
    assertEquals(1, builderLists.size());
    BuilderList lastBuild = getLastBuild(builderLists);
    assertEquals(""test"", lastBuild.getName());
    assertEquals(2, logs.size());
}
```

With BDD:

```java
@Test
public void shouldGenerateAsyncMessagesFromInterface() {
    Interface messageInterface = Interfaces.createFrom(SimpleMessageService.class);
    PublisherInterface publisher = new PublisherInterface(messageInterface, transport);

    // When we invoke a method on the interface
    SimpleMessageService service = publisher.createPublisher();
    service.sendMessage(""Hello"");

    // Then a message should be sent through the transport
    verify(transport).send(argThat(message ->
        message.getMethod().equals(""sendMessage"") &&
        message.getArguments().get(0).equals(""Hello"")
    ));
}
```",1746631566.0,computerscience,STEM
1kfqvuy,is graph theory a good expertise in computer science,"i really enjoy graph theory problems and the algorithms associated with them. i guess my question is, would becoming proficient in this theory be useful? i haven’t really found a branch of comp sci to “expertise” in and was looking for perspectives.",1746489594.0,computerscience,STEM
1kf9xut,I need an efficient data-structure to do index-based range-searches over mutable records,"The use-case is that I want to add records with a certain weight and do random picks from the map with these weights driving the probabilities of picking a specific record. This would be easy enough to do, but these records need to be mutable and since it's going to be both very busy and very big (hundreds of millions of records), resizing the complete index on each modification is out of the question.

This structure is expected to be very big and busy.

  
So, to put it differently: If I have the elements A, B, C and D with the (respective) relative weights of 1, 2, 3, 4, the chances of picking A will be 1:10 (10=1+2+3+4). If I then remove B, the chances of picking A will be 1:8. I'm thinking if something like this doesn't exist already (as is) I could go with some kind of cross between a b-tree and a trie, where we would have multi-level indexes, but where the reading process needs to add up the values of the keys along the way, to know if they should move sideways or deeper in the tree.",1746446739.0,computerscience,STEM
1kewsdw,Computer Networking Resources,"Hello buddies,

Is there a computer networks resource that isn't actually garbage?

Let me explain. I am about to graduate in Math and CS and my uni kind of failed me on the systems side. I took your typical Computer Systems - Networks - Operating Systems classes but, by luck or otherwise, these 3 were taught on a lecturer-reading-slides way. 

Now, about to get my diploma, I'm clueless about networks. Is there a nice book, youtube lecture series, or something, that actually teaches you networks in the same way that other courses would teach you something hands-on? Even if theoretical? Here are some examples of what I mean. 

Algorithms is hands on: problem sets that asks you to proof correctness of algorithms, computing complexity, coming up with variations of algos to solve a problem. 

Data Structures is hands on: code the structures from scratch on c++.

ML is hands on: get a dataset and build a model that classifies well

SWE is hands on: Read an architecture pattern and code something with it

Math is hands on: literally just do problem sets

What resources is hands-ons in networking? I don't want to memorize that the TCP header is 8 bytes (or whatever size it is) without ever looking at it beyond the silly graph in your usual textbook. I want to solve some problems, code something up, do something. Kurose's book problem, skimming through them, feel more like High School trivia, though I might be wrong. Any help is most welcomed.",1746399206.0,computerscience,STEM
1ke1xy4,Flow network - residual graphs,"I’m sorry if this isn’t the correct place to ask such a question but I didn’t this exactly breaking the rules.  I’m currently studying for my algorithms final tomorrow and I’ve been conceptually struggling to understand the role of the residual graph and residual paths in finding the max-flow. 

In the graph attached, when using the Ford Fulkerson algorithm with DFS, in the worst case a flow of 1 is pushed through the augmenting path repeatedly in an oscillating manner. What I’m struggling to understand is why, after the very first time that the augmenting path is found and a flow of 1 is pushed through it, causing the flow to equal capacity through the middle edge, we are still able to find the same augmenting path again and again and pass flow through it.  

I’d really appreciate any help! Thanks a lot. ",1746302961.0,computerscience,STEM
1kdwc4c,"Relation between API, driver and firmware","What is the relation between API, driver and firmware? From what I understand API is the intermediate between the application and the driver, the driver gives the low level instructions and firmware does what?",1746288122.0,computerscience,STEM
1kcigvx,Ford-Fulkerson Algorithm: A Step-by-Step Guide to Max Flow,,1746130557.0,computerscience,STEM
1kc83d1,How to count without the side effect caused by float precision of decimal numbers ?,"Given two arbitrary vectors, which represent a bounding box in 3D space . They represent the leftbottom and the righttop corners of a box geometry . My question is , I want to voxelize this bounding box, but I can't get a correct number of total number of boxes .

To elaborate : I want to represent this bounding volume with several little cubes of constant size . And they will be placed along each axis with different amounts per axis. This technically would be easy but soon I encountered the problem of float precision . As decimal numbers are represented with negative powers, you have to fit the numerical value . Binary representation cannot represent it easily . It's like binary tree that you divide the whole tree into ""less than 0.5"" and ""greater than 0.5"" . After that , you divide each parts into 0.25 and 0.75. You repeat this process and finally get an approximate value .

The problem is : ***ceil((righttop.x-leftbottom.x)/cubesize)*** outputs 82 while ***ceil(righttop.x/cubesize)-ceil(leftbottom.x/cubesize)*** outputs 81 because ***(righttop.x-leftbottom.x)/cubesize*** equals to 81.000001 which is ceiled to 82, while I was expecting it to be ***ceil(81.000001)==81*** .

How should you calculate it in this case ?",1746104331.0,computerscience,STEM
1kbo001,I built a toy to help learn about arrays and pointers,"Sometimes, I get sad that most of what I build are just metaphors for electrons occupying different spaces--so I start picturing tactile representations.  Here is one I designed in Fusion for Arrays and pointers. 

It helped with explaining the concept to my 10 year old--although it didn't much help with the ""but why?"" question.",1746038512.0,computerscience,STEM
1kcsnan,"I accidentally figured out a way to calculate 100,000 digits of pi in 14 seconds 💀","I was trying to substitute pi without using pi, from a trigonometric identity, after trying a lot it gave me PI=2[1+arccos(sin(1))], I tried it in code, making it calculate 100 thousand digits of pi, and that is, it calculated it in 14.259676218032837 seconds, and I was paralyzed 💀

Heres the code:
```
import mpmath

# Set the precision to 10,000 decimal digits
mpmath.mp.dps = 100000

# Calculate the value of x2 = 2 * (1 + arccos(sin(1)))
sin_1 = mpmath.sin(1)
value = mpmath.acos(sin_1)
x2 = 2 * (1 + value)

# Display the first 1000 digits for review
str_x2 = str(x2)
str_x2[:1000]  # Show only the first 1000 characters to avoid overwhelming the screen
```

Heres the code for know how many time it takes:
```
import time
from mpmath import mp, sin, acos

# Set precision to 100,000 digits
mp.dps = 100000

# Measure time to calculate pi using the sin(1) + acos method
start_time = time.time()
pi_via_trig = 2 * (1 + acos(sin(1)))
elapsed_time = time.time() - start_time

# Show only the time taken
elapsed_time

```
",1746160861.0,computerscience,STEM
1kbmsen,From Data to Display: How Computers Present Images,"Most of us use technological devices daily, and they're an indispensable part of our lives. A few decades ago, when the first computer came up, the screen only displayed black and white colors. Nowadays, from phones to computers to technical devices, the colorful display is what we take for granted. But there is one interesting question from a technical perspective: if the computer can only understand zeros and ones, then how can a colorful image be displayed on our screen? In this blog post, we will try to address this fundamental question and walk through a complete introduction to the image rendering pipeline, from an image stored in memory to being displayed on the screen

https://preview.redd.it/xkpr3pzv889f1.png?width=1808&format=png&auto=webp&s=a5cb7c6c88565d6e010363e799cbae35a1244ea1

[https://learntocodetogether.com/image-from-memory-to-display/](https://learntocodetogether.com/image-from-memory-to-display/)",1746035521.0,computerscience,STEM
1kaz67b,About how many bits can all the registers in a typical x86 CPU hold?,"I know you can't necessarily actually access each one, but I was curious how many registers there are in a typical x86 processor (let's say a 4 core i7 6820 hq, simply cause it's what I have). I've only found some really rough guestimates of how many registers there are from Google, and nothing trying to actually find out how big they are (I don't know if they're all the same size or if some are smaller). Also, I was just curious which has more space, the registers in my CPU or a zx spectrums ram, because just by taking the number this thread ( https://www.reddit.com/r/programming/comments/k3wckj/how_many_registers_does_an_x8664_cpu_have/ )suggests and multiplying it by 64 then 4 you actually get a fairly similar value to the 16kb a spectrum has",1745961260.0,computerscience,STEM
1kb3bea,Is Linear Probing Really that Bad of a Solution for Open-Addressing?,"I've been watching several lectures on YouTube about open addressing strategies for hash tables. They always focus heavily on the number of probes without giving much consideration to cache warmth, which leads to recommending scattering techniques like double hashing instead of the more straightforward linear probing. Likewise it always boils down to probability theory instead of hard wall clock or cpu cycles.

Furthermore I caught an awesome talk on the cppcon channel from a programmer working in Wall Street trading software, who eventually concluded that linear searches in an array performed better in real life for his datasets. This aligns with my own code trending towards simpler array based solutions, but I still feel the pull of best case constant time lookups that hash tables promise.

I'm aware that I should be deriving my solutions based on data set and hardware, and I'm currently thinking about how to approach quantitative analysis for strategy options and tuning parameters (eg. rehash thresholds) - but i was wondering if anyone has good experience with a hash table that degrades to linear search after a single probe failure? It seems to offer the best of both worlds.

Any good blog articles or video recommendations on either this problem set or related experiment design and data analysis? Thanks.",1745972524.0,computerscience,STEM
1kbsnne,Is this course interesting?,"Hey guys so im a cs student and while i dont know much of the field, im interested in cybersecurity and would like to try out a course in it.
I found a course (course description below) that seems interesting and i wanted to ask about it. Does it cover stuff that would be both interesting and important to know? I got a small sense of the what it covers and talked with the professor but its still pretty vague for me. just wanted to ask around

Course Description:
This course provides students with an in-depth understanding of the principles, methodologies and tools of Digital Forensics and Incident Response. It covers a wide range of forensic investigations including filesystem, memory, network and mobile forensics. Students will engage in hands-on labs using industry-standard tools and methodologies to investigate security incidents, recover digital evidence and prepare reports that are admissible in court. The course will cover forensic investigation techniques for various platforms and applications, as well as proper incident response procedures. The course emphasizes practical application including the handling of evidence, analysis of data and effective incident response in various environments. Students will learn how to create a digital forensics workstation and conduct investigation on practical cases. Additionally, the course places significant emphasis on the ethical and legal dimensions of digital forensics, ensuring that students can produce detailed forensic and incident response reports that document each stage of the investigation in adherence to legal and professional standards. ",1746050384.0,computerscience,STEM
1kaigyx,What are the Implications of P=NP?,"I am trying to write a sci-fi thriller where in 2027, there are anomalies in the world which is starting to appear because someone proves P=NP in specific conditions and circumstances and this should have massive consequences, like a ripple effect in the world. 
I just want to grasp the concept better and understand implications to write this setting better. 
I was thinking maybe one of the characters ""solves"" the Hodge conjecture in their dream and claims they could just ""see"" it ( which btw because a scenario where P=NP is developing) and this causes a domino effect of events. 

I want to understand how to ""show"" Or depict it in fiction, for which I need a better grasp

 thanks in advance for helping me out. ",1745913003.0,computerscience,STEM
1kanizq,CS Education Research,"What's your view on CS Ed research? After working in CS Ed, what are the chances of getting hired as a teaching professor? Do you think the demand for CS will keep growing? Or it's a risky gamble? Cause if the demand shrinks, the need for CS Ed professors may shrink too. I enjoy the work, but future employability is becoming a bigger issue. ",1745932133.0,computerscience,STEM
1ka1y8r,How do Single Core Processors Handle Concurrent Processes?,"I watched some videos on YouTube and found out that programs and processes often don't use the CPU the entire time. A process will need the CPU for ""CPU bursts"" but needs a different resource when it makes a system call.

Some OS like MS-DOS were non-preemptive and waited for a process to finish its CPU burst before continue to the next one. Aside from not being concurrent if one process was particularly CPU hungry, if it had an infinite loop, this would cause process starvation. More sophisticated ones like Windows 95 and Mac OS would eventually stop a process using the CPU and then move on to another process. So by rapidly switching between multiple processes, the CPU can handle concurrent processes.

My question is how does the processor determine what is a good time to kick out a still running process? If each process is limited to 3 milliseconds, then most of the CPU time is spent swapping between processes and not actually running them. If it waits 3000 milliseconds before swapping, then the illusion of concurrently running programs is lost. Is the maximum time per process CPU (hardware) dependent? OS (Software) dependent? If it is a limit per process of each CPU, does the manufacturer publish the limit?",1745862880.0,computerscience,STEM
1ka0ouk,Embed graph with fixed-length edges on a square grid,,1745859861.0,computerscience,STEM
1k94pqk,What happens if P=NP?,No I don’t have a proof I was just wondering,1745762684.0,computerscience,STEM
1k93ewk,Computer Science book that will lead to insights into various Computer Systems?,"Is there a book out there that would provide an overview of all CS that would come in handy when trying to understand things like containers, network architecture, python scripts, database replication, devops, etc? I was thinking about going through Nand2Tetris but that seems like it might be more low-level than I'd need to get the information I'm looking for. Unless you think a computer architecture and systems programming book like that would prove to be useful. Thank you for your help.",1745758844.0,computerscience,STEM
1k8imyd,Transition to system programming and distributed systems,"I've a background in full stack development and smart contract development. But it's not fulfilling for me because I love difficult tasks and challenges, and what I was doing feel really shallow. 

My goal is to become a good systems programmer as well as distributed systems engineer. But I lack necessary skills to achieve my goals because my fundamentals aren't strong.

So I decided to read ""Code: Hidden Language"" by charles petzold, and after that I want to complete nand2tetris. I'll jump into C language, will create some projects, and then will learn Rust. 

To become a good engineer, I think it's better if you have solid basic concepts. That's why I started to read the book and will follow the course.

I want to do it full-time because it will be done sooner and without any distraction. Also context switching is a huge problem for me. So I want to focus completely on this roadmap. 

The question is, am I missing something? Am I overthinking it? Is it a good roadmap? ",1745689321.0,computerscience,STEM
1k8i5iw,Resources on combinatorics or discrete math in general,"My ultamite goal is to be good at DSA. So, I'm trying to learn combinatorics from scratch, i have no idea what does it mean so far. I heard it's really important for my cs education. How to start? any courses or books that start from scratch and then dive deep. Are there any prerequisites i should learn before getting started with it? should i start with proofs and discrete math, set theory before it?",1745688073.0,computerscience,STEM
1k7q893,"What,s actually in free memory!","So let’s say I bought a new SSD and installed it into a PC. Before I format it or install anything, what’s really in that “free” or “empty” space? Is it all zeros? Is it just undefined bits? Does it contain null? Or does it still have electrical data from the factory that we just can’t see?",1745600980.0,computerscience,STEM
1k70ncf,My Computer Science final said CDs are not storage?,"Aren’t they? They store files by definition…the question was “blue ray discs and CDs are examples of storage devices” I selected true but got the question wrong. Worth messaging teacher? I also was asked if a smart watch was a Ubiquitous computer and said yes but that also came back as wrong. After the test I looked up both things and it says I’m correct. Are these debatable topics? Could my teacher have a reason or did I miss something in the way it was asked? 

Is this worth sending a message to him for? 


Edit: I did message him for clarity with the understanding I may be incorrect based on technicalities and opinion! I actually am really enjoying this post now because it’s brought up a rather interesting debate on something I didn’t think too deeply about! 


Update a few days later or a week idk: My teacher responded to my message that I did miss the media vs device distinction. However he actually did change my grade because he agreed that a wearable watch could be classified as Ubiquitous. I would like to think messaging him was worth it! Thank you to everyone who commented and contributed to the discussion :) ",1745521929.0,computerscience,STEM
1k77ekx,(Why) are compilers course practicums especially difficult?,"In more than one (good) academic institution I've taken a compilers course at, students or professors have said ""this course is hard,"" and they're not wrong.

I have no doubt it's one of the best skills you can acquire in your career. I just wonder if they are inherently more difficult than other practicums (e.g.  databases, operating systems, networks).

Are there specific hurdles when constructing a compiler that transcends circumstantial factors like the institution, professor that are less of a problem with other areas of computer science?",1745539668.0,computerscience,STEM
1k80qrp,[Some CS Maths] [a JWL Paper] Concerning A Special Summation That Preserves The Base-10 Orthogonal Symbol Set Identity In Both Addends And The Sum,"**INVITING** early readers, reviewers, fellow researchers, academicians, scholars, students & especially the mathematical society, to read, review & apply the important ideas put forward in [Fut. Prof.] JWL's paper on the mathematics of symbol sets: https://www.academia.edu/resource/work/129011333

-----|

**PAPER TITLE:** Concerning A Special Summation That Preserves The Base-10 Orthogonal Symbol Set Identity In Both Addends And The Sum

**ABSTRACT:**
While working on another paper (yet to be published) on the matter of random number generators and some number theoretic ideas, the author has identified a very queer, but interesting summation operation involving two special pure numbers that produce another interesting pure number, with the three numbers having the special property that they all preserve the orthogonal symbol set identity of base-10 and $\psi_{10}$. This paper formally presents this interesting observation and the accompanying results for the first time, and explains how it was arrived at --- how it can be reproduced, as well as why it might be important and especially unique and worthy or further exploration.

**KEYWORDS:** Number Theory, Symbol Sets, Arithmetic, Identities, Permutations, Magic Numbers, Cryptography

**ABOUT PAPER:** Apart from furthering (with 4 new theorems and 9 new definitions) the mathematical ideas concerning symbol sets for numbers in any base that were first put forward in the author's GTNC paper from 2020, this paper presents some new practical methods of generating special random numbers with the property that they preserve the base-10 o-SSI.

#Research #ResearchPaper #NumberTheory #SymbolSets #MagicNumbers #Cryptography #ProfJWL #Nuchwezi #ComputerScience #Preprints

**DOI:** 10.6084/m9.figshare.28869755",1745628933.0,computerscience,STEM
1k731bd,Computer science books and roadmaps,"Hi all, I want to achieve a deeper understanding of computer science that goes beyond software eng. Could you share books that I should read and are considered “bibles” , roadmaps and suggestions? I am a physicist working at the moment as data eng",1745527774.0,computerscience,STEM
1k7euco,Research paper help,"Hello guys , I recently co wrote a research paper on Genetic algorithms and was searching for conferences to publish in India which will take place before Sept 2025 as am leaving for my masters . So if you have any leads about any good conferences about computer science during that time kindly please do share , its urgent .  
",1745565056.0,computerscience,STEM
1k64jxe,Computer science theory wins you’ve actually used for prep,"We all learned heaps of algorithm / automata theory, but how often do you really deploy it?

My recent win: turned a gnarly string‑search bug into a clean Aho‑Corasick automaton cut runtime from 45 s ➜ 900 ms.  

A teammate used max‑flow / min‑cut to optimize a supply‑chain model, saving the client \~$40 k/mo.



Drop your stories (and what course prepped you). Bonus points if the professor swore “you’ll use this someday”… and they were right.  

",1745428198.0,computerscience,STEM
1k6pshs,What is oflag in Unix system calls?,"Hi, i'm trying to search information about this but Is very hard. So what is oflag? For example the system call open requires a string of char for the directory, and an int oflag. But the flags are like: O_RDONLY, O_WRONLY... so how it can be an integer? I have seen that the file permissions are represented by a string with 3 3-bit triplets (the first for user permission)but i don't have any clear study material on these topics. Thanks for the help
",1745493936.0,computerscience,STEM
1k69z70,Data reconstruction from machine learning models via inverse estimation and Bayesian inference,,1745441312.0,computerscience,STEM
1k5z8mg,How to,"So, I've been wanting to get into cs for a while now, not really had any idea where to start as it seemed abit too much, some people recommended learning binary code and a few other random things, how should I be introduced to computer science/programming? Any books you guys could recommend? Any sites etc.  ",1745415080.0,computerscience,STEM
1k4wka0,"Is this correct? If not, how would you make it correct?",,1745292059.0,computerscience,STEM
1k4usm2,Did we miss something focusing so much on Turing/Von Neumann style computers?,"I know that quantum computers have been around for a little while, but that's not what I'm talking about.
I'm talking about perhaps an alternative classical computer. What would we have come up with if we didn't have Turing or Von Neumann?
Was there a chance it'd be better or worse?
I know Turing was one monumentally brilliant man, I'm just not sure if we could've done any better.

edit: Why are you guys upvoting this. I've come to realize this is a very stupid question.",1745286554.0,computerscience,STEM
1k4i4az,"Wild how many people in a OpenAI subreddit thread still think LLMs are sentient, do they even know how transformers work?",,1745253983.0,computerscience,STEM
1k4njwn,ELI5: What is OAuth?,"So I was reading about OAuth to learn it and have created this explanation. It's basically a few of the best I have found merged together and rewritten in big parts. I have also added a super short summary and a code example. Maybe it helps one of you :-) This is the [repo][ref2].

# OAuth Explained

## The Basic Idea

Let’s say LinkedIn wants to let users import their Google contacts.

One obvious (but terrible) option would be to just ask users to enter their Gmail email and password directly into LinkedIn. But giving away your actual login credentials to another app is a huge security risk.

OAuth was designed to solve exactly this kind of problem.

Note: So OAuth solves an authorization problem! Not an authentication problem. See [here][ref1] for the difference.

## Super Short Summary

- User clicks “Import Google Contacts” on LinkedIn
- LinkedIn redirects user to Google’s OAuth consent page
- User logs in and approves access
- Google redirects back to LinkedIn with a one-time code
- LinkedIn uses that code to get an access token from Google
- LinkedIn uses the access token to call Google’s API and fetch contacts

## More Detailed Summary

Suppose LinkedIn wants to import a user’s contacts from their Google account.

1. LinkedIn sets up a Google API account and receives a client_id and a client_secret
   - So Google knows this client id is LinkedIn
2. A user visits LinkedIn and clicks ""Import Google Contacts""
3. LinkedIn redirects the user to Google’s authorization endpoint:
   https://accounts.google.com/o/oauth2/auth?client_id=12345&redirect_uri=https://linkedin.com/oauth/callback&scope=contacts

- client_id is the before mentioned client id, so Google knows it's LinkedIn
- redirect_uri is very important. It's used in step 6
- in scope LinkedIn tells Google how much it wants to have access to, in this case the contacts of the user

4. The user will have to log in at Google
5. Google displays a consent screen: ""LinkedIn wants to access your Google contacts. Allow?"" The user clicks ""Allow""
6. Google generates a one-time authorization code and redirects to the URI we specified: redirect_uri. **It appends the one-time code as a URL parameter**.
   - So the URL could be https://linkedin.com/oauth/callback?code=one_time_code_xyz
7. Now, LinkedIn makes a server-to-server request (not a redirect) to Google’s token endpoint and receive an access token (and ideally a refresh token)
8. **Finished**. Now LinkedIn can use this access token to access the user’s Google contacts via Google’s API

---

**Question:**
_Why not just send the access token in step 6?_

**Answer:** To make sure that the requester is actually LinkedIn. So far, all requests to Google have come from the user’s browser, with only the client_id identifying LinkedIn. Since the client_id isn’t secret and could be guessed by an attacker, Google can’t know for sure that it's actually LinkedIn behind this. In the next step, LinkedIn proves its identity by including the client_secret in a server-to-server request.

## Security Note: Encryption

OAuth 2.0 does **not** handle encryption itself. It relies on HTTPS (SSL/TLS) to secure sensitive data like the client_secret and access tokens during transmission.

## Security Addendum: The state Parameter

The state parameter is critical to prevent cross-site request forgery (CSRF) attacks. It’s a unique, random value generated by the third-party app (e.g., LinkedIn) and included in the authorization request. Google returns it unchanged in the callback. LinkedIn verifies the state matches the original to ensure the request came from the user, not an attacker.

## OAuth 1.0 vs OAuth 2.0 Addendum:

OAuth 1.0 required clients to cryptographically sign every request, which was more secure but also much more complicated. OAuth 2.0 made things simpler by relying on HTTPS to protect data in transit, and using bearer tokens instead of signed requests.

## Code Example: OAuth 2.0 Login Implementation

Below is a standalone Node.js example using Express to handle OAuth 2.0 login with Google, storing user data in a SQLite database.

```javascript
const express = require(""express"");
const axios = require(""axios"");
const sqlite3 = require(""sqlite3"").verbose();
const crypto = require(""crypto"");
const jwt = require(""jsonwebtoken"");
const jwksClient = require(""jwks-rsa"");

const app = express();
const db = new sqlite3.Database("":memory:"");

// Initialize database
db.serialize(() => {
  db.run(
    ""CREATE TABLE users (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, email TEXT)""
  );
  db.run(
    ""CREATE TABLE federated_credentials (user_id INTEGER, provider TEXT, subject TEXT, PRIMARY KEY (provider, subject))""
  );
});

// Configuration
const CLIENT_ID = process.env.GOOGLE_CLIENT_ID;
const CLIENT_SECRET = process.env.GOOGLE_CLIENT_SECRET;
const REDIRECT_URI = ""https://example.com/oauth2/callback"";
const SCOPE = ""openid profile email"";

// JWKS client to fetch Google's public keys
const jwks = jwksClient({
  jwksUri: ""https://www.googleapis.com/oauth2/v3/certs"",
});

// Function to verify JWT
async function verifyIdToken(idToken) {
  return new Promise((resolve, reject) => {
    jwt.verify(
      idToken,
      (header, callback) => {
        jwks.getSigningKey(header.kid, (err, key) => {
          callback(null, key.getPublicKey());
        });
      },
      {
        audience: CLIENT_ID,
        issuer: ""https://accounts.google.com"",
      },
      (err, decoded) => {
        if (err) return reject(err);
        resolve(decoded);
      }
    );
  });
}

// Generate a random state for CSRF protection
app.get(""/login"", (req, res) => {
  const state = crypto.randomBytes(16).toString(""hex"");
  req.session.state = state; // Store state in session
  const authUrl = `https://accounts.google.com/o/oauth2/auth?client_id=${CLIENT_ID}&redirect_uri=${REDIRECT_URI}&scope=${SCOPE}&response_type=code&state=${state}`;
  res.redirect(authUrl);
});

// OAuth callback
app.get(""/oauth2/callback"", async (req, res) => {
  const { code, state } = req.query;

  // Verify state to prevent CSRF
  if (state !== req.session.state) {
    return res.status(403).send(""Invalid state parameter"");
  }

  try {
    // Exchange code for tokens
    const tokenResponse = await axios.post(
      ""https://oauth2.googleapis.com/token"",
      {
        code,
        client_id: CLIENT_ID,
        client_secret: CLIENT_SECRET,
        redirect_uri: REDIRECT_URI,
        grant_type: ""authorization_code"",
      }
    );

    const { id_token } = tokenResponse.data;

    // Verify ID token (JWT)
    const decoded = await verifyIdToken(id_token);
    const { sub: subject, name, email } = decoded;

    // Check if user exists in federated_credentials
    db.get(
      ""SELECT * FROM federated_credentials WHERE provider = ? AND subject = ?"",
      [""https://accounts.google.com"", subject],
      (err, cred) => {
        if (err) return res.status(500).send(""Database error"");

        if (!cred) {
          // New user: create account
          db.run(
            ""INSERT INTO users (name, email) VALUES (?, ?)"",
            [name, email],
            function (err) {
              if (err) return res.status(500).send(""Database error"");

              const userId = this.lastID;
              db.run(
                ""INSERT INTO federated_credentials (user_id, provider, subject) VALUES (?, ?, ?)"",
                [userId, ""https://accounts.google.com"", subject],
                (err) => {
                  if (err) return res.status(500).send(""Database error"");
                  res.send(`Logged in as ${name} (${email})`);
                }
              );
            }
          );
        } else {
          // Existing user: fetch and log in
          db.get(
            ""SELECT * FROM users WHERE id = ?"",
            [cred.user_id],
            (err, user) => {
              if (err || !user) return res.status(500).send(""Database error"");
              res.send(`Logged in as ${user.name} (${user.email})`);
            }
          );
        }
      }
    );
  } catch (error) {
    res.status(500).send(""OAuth or JWT verification error"");
  }
});

app.listen(3000, () => console.log(""Server running on port 3000""));
```

[ref1]: https://stackoverflow.com/questions/6556522/authentication-versus-authorization
[ref2]: https://github.com/LukasNiessen/oauth-explained",1745266844.0,computerscience,STEM
1k4fuc9,Doubt in Dsa,"Guys, while traversing a directed graph using BFS or DFS, some nodes may not be reachable. What should we do in that case? Is it okay to leave ?",1745248059.0,computerscience,STEM
1k4f7le,Swift or Python for teaching 16+ Programming?,"I come to teaching FE from a React/Node/PHP background and have been looking at Swift recently. Its ability to explicitly type variables seems to be a big win over the current A'Level favourite of Python which is hideously loosely typed. As most of the examining boards do not mandate a specific language, I'm wondering if I shouldn't be arguing for the introduction of Swift as a language for us to teach across multiple platforms, and even easily incorporate UI Apps for students to see beynd the command line. What do other teachers of programming think?",1745246513.0,computerscience,STEM
1k411lr,Typical computer speeds,"Hi everyone,

I understand that most modern processors typically run at speeds between 2.5 and 4 GHz. Given this, I'm curious why my computer sometimes takes a relatively long time to process certain requests. What factors, aside from the CPU clock speed, could be contributing to these delays?",1745196474.0,computerscience,STEM
1k3vp73,Byzantine Fault Tolerance: How Computers Trust Each Other When They Shouldn't,"Wanted to share this cool concept called Byzantine Fault Tolerance (BFT). It tackles one of distributed computing's toughest challenges: how do computers reach agreement when some nodes might be sending contradictory information to different parts of the system? Named after the Byzantine Generals' Problem, these algorithms ensure systems keep working correctly even when up to a third of nodes are compromised or malfunctioning. Air traffic control systems use BFT principles to make critical decisions when some radar inputs might be giving false readings. Distributed databases rely on BFT for syncing state. Same thing with blockchains. The list goes on... 

One game changer was the Practical Byzantine Fault Tolerance algorithm developed in 1999 (https://pmg.csail.mit.edu/papers/osdi99.pdf), which made these systems actually implementable in the real world. Before that, the communication overhead was too massive to be useful. Now BFT principles protect everything from cloud databases to financial networks, creating systems that don't just detect failures but can continue operating reliably through them. 

For more on this by the legend leslie lamport himself: [https://lamport.azurewebsites.net/pubs/byz.pdf](https://lamport.azurewebsites.net/pubs/byz.pdf)",1745180836.0,computerscience,STEM
1k4ai8p,Is this B-Tree insertion process correct?,"I came across this solution in a past exam for an algs & data structures course, and really struggling to see how they got this answer. It's my understanding that the one-pass approach to B-Tree insertions means that you would split any **full nodes** that are encountered during traversal to the insertion node, but at step 3 (insertion of 15) and step 7 (insertion of 13), the solution shows a split of nodes with *2t-2* keys, which are not yet full. 

  
My solution would be to end up with a tree that has \[12\] as the root, with \[4,6,7\] and \[13,14,15\] as l/r children respectively. Unless I'm missing something, this would meet all the b-tree properties for t=2, and have half the height of this past paper solution.

  
What am I missing? Many thanks!",1745232340.0,computerscience,STEM
1k3yv31,What are some efficient optimal algorithms for the Traveling Salesperson problem?,I have been scouring the internet for a better solution to this problem but everything I can find is either O(n!) in the worst case or relies on just being “good enough.”I realize that being close enough for this problem is more than sufficient for most real-world cases but I’m looking for exact solutions. Is there really nothing better than factorial?,1745189872.0,computerscience,STEM
1k3ep22,Any computer networking textbooks you'd recommend for teaching to highschool?,Pretty much what the title says. I need something the kids can read from and not run away as soon as they see the first acronym.,1745123956.0,computerscience,STEM
1k23e4m,fully understanding computers and internet,"hi, all. I would like to fully understand computers and internet and how it all functions and not just on a surface level like what each part does, or something like that. I want to be able to break it down until I can't anymore, only because there isnt really anything left, not because of limited knowledge; and I don't really know where to start, hence my post here: so I'm looking for directions.
It would be great if anyone could give me a list of materials and whatever other word of advice, thanks :D",1744978231.0,computerscience,STEM
1k1shwf,Why do video game engines use floats rather than ints (details of question in body),"So the way it was explained to me, floats are prefered because they allow greater range, which makes a lot of sense.

Reasonably, in most games I imagine that the slowest an object can move is the equivalent of roughly 1 mm/second, and the fastest is equivalent to probably maximum bullet velocity,  roughly 400 meter/second, i.e. 400,000 mm/second. This suggests that integers from 1 to 400,000 cover all reasonable speed ranges, i.e. 19 bits, and even if we allowed much greater ranges of numbers for other quantities, it is not immediately obvious to me why one would ever exceed a 32-bit signed integer, let alone a 64-bit int.

I'm guessing that this means that there are other considerations at play that I'm not taking into account. What am I missing folks?

  
EDIT: THANK EVERYBODY FOR THE DETAILED RESPONSES!",1744937509.0,computerscience,STEM
1k1l6zm,How to deal with outliers in RL,"Hello,

I'm currently dealing with RL on a CNN for which a have 50 input images, which I scaled up to 100.

The environment now, which consists of an external program, doesn give a feedback if there are too many outliers among the 180 outputs.

I'm trying so use a range loss which basically is function of the difference to the closer edge.

The problem is that I cannot observe a convergence to high rewards and the outliers are getting more and more instead of decreasing.

Are there propper methods to deal with this problem or do you have experience?",1744917520.0,computerscience,STEM
1jz4oqu,Computer Science Roadmap,"https://roadmap.sh/computer-science

What do you think about this roadmap? I feel like this isn't enough. Because I couldn't see lessons for math, physics, computer architecture, operating systems etc. I'm new to this, so I accept any kind of comments :D",1744651881.0,computerscience,STEM
1jzimer,Language-Independent Dynamic Dependency Visualizer,"Hi everyone,

Wanted to push out an idea I had with the main goal of learning some cool new things and creating something somewhat useful. I still have a lot of research to do on existing tools and ideas but wanted to discuss on this sub to see if there was anyone who had built something similar, had any tips, or would like to possibly collaborate.

The main goal would be to create a tree visualization of dependencies in a codebase. As far as granularity, I would like to start with source file dependencies on each other and then move to function or class-level dependencies once something’s going. The input would simply be the root directory of some codebase and the output would be said tree visualization. 

Few things I’d like to emphasize. I plan to make it dynamic - given the initialization of this visualizer in the root, i would like to be able to make changes and leverage source control to easily reflect the state of dependencies at any point. I also hope to make it language-independent (or at least cross language for a large variety of languages) - the most straightforward though most tedious would likely be casework based on file extension with language-specific parsers for retrieving dependency info per language. I’d guess that true language independence would be a very, very difficult task but not really sure if I’m taking on something way over my head. Lastly, I hope to make it IDE-independent and run completely in a shell environment to work directly with the file system.

I’ve heard of things like sourcegraph and C# dependency visualizers that do sort of the same thing but lack one or a few aspects I mentioned above. Please feel free to tell me if I’m being overly ambitious here or of thoughts y’all might have, thanks!",1744689571.0,computerscience,STEM
1jzd59l,Books or resources for a Jr. MLE?,"Ive already graduated, been a Jr. MLE for 8 months and i want to keep perfecting my skills, however all books or resources ive seen recomended on the internet are for example if i wanted to learn how run; the books would goo into great detail about the quadricep muscle, but nothing about running itself.

I want to learn more advance stuff of how to put everything together, not learn another python library by itself, Any recomendations?",1744673041.0,computerscience,STEM
1jz8yrj,What you guys think about Clound Computing?,"I'm learning about this and I still don't get about it.
I want to know more about this",1744662117.0,computerscience,STEM
1jy8t8g,relating all concepts you learn from different streams of science,"im a freshman in CS and currently i have five classes OOP(java), Database systems, Digital Logic design, Discrete Mathematics and Calculus. in last sem we did C++ fundamentals, ICT, precalc. the thing is i was wondering if its possible to connect all of the concepts im learning or have learned. its so confusing idk how to explain but basically we have concepts in Discrete Maths and DLD which overlap but i cannot figure out a way to do it. like create a single interrelated network /web of all the interrelated stem fields where i can add new concepts as i learn them. kind of like a murdermap. i just wanted to know if itd be possible or if anyone has tried doing it or if its too stupid of an idea ",1744554253.0,computerscience,STEM
1jy0nay,Any application of Signals and Systems?,"I am interested in learning more about the subject of image processing/computational imaging. For reference, I have/am planning to take college courses in Computer Graphics, Computer Vision, and ML. Is there any use for me to take a semester to learn the math of Signals and Systems, where I will not (formally) learn specifically about Digital Signal Processing? It's a field I'm curious about, but not dead set on. And I'd rather not waste my time on something if I likely am not going to be using it ever/learning a lot more information (Analog DS) than I need to.

What background would I want to know for Image Processing. Would it need to be a lot of math like S&S?

Going to say (for the mods) that I hope this doesn't go against rule 3 since it's more about the application of a subject in CS than classes specifically.",1744522744.0,computerscience,STEM
1jy6u9u,Are Devs Actually Ready for the Responsibility of Handling User Data?,"Are devs truly ready to handle the gigantic responsibility that comes with managing user data in their apps? Creating apps for people is awesome, but I'm a bit skeptical. I mean, how many of us are REALLY prepared for all that responsibility? We dive into our projects with passion, but are most devs fully conscious of what they're getting into when it comes to data implications? Do we really know enough about authentication and security to protect user data like we should? Even if you're confident with tech, it's easy to underestimate the implications or just assume, ""It won't happen to me."" It’s not just the tech part, either. There’s a whole ethical minefield connected to handling this stuff. So... how do you guys tackle this? When a developer creates an app that relies on user-provided data, everything might seem great at the launch—especially if it's free. But then, the developer becomes the person in charge of managing all that data. With great power comes great responsibility, so how does one handle that? My biggest fear is feeling ready to release something, only to face some kind of data leakage that could have legal consequences.",1744548389.0,computerscience,STEM
1jxpua9,A Computational Graph builder for circuit evaluation and constraint checking,"Built a library for constructing computational graphs that allows you to represent any function or computational circuit as a graph and run evaluations on it or specific constraint checks. This is very relevant in the area of verifiable computation and zero knowledge proofs. A lot of the algorithms in that realm usually require you to represent whatever function/computation you're evaluating as a graph which you can then evaluate constraints, etc. I've been wanting to write a bunch of these proof systems from scratch so built this as a primitive that I can use to make things easier.

The algorithm I wrote creates a level for each arithmetic operation starting from the input nodes. The evaluation and constraint checking is then performed in a sorted manner for each level, and is parallelized across all the nodes in a given level. Constraints are also checked once all the nodes involved in that constraint have computed values. I wrote it in Rust :) 

I provided a few examples in the readme: [https://github.com/AmeanAsad/comp-graph/blob/main/README.md](https://github.com/AmeanAsad/comp-graph/blob/main/README.md)



  


",1744487600.0,computerscience,STEM
1jxopcz,Why electrons flow from the N-semiconductor to a P-semiconductor?,"Suppose we have an NP-semiconductor. From what I understand, electrons flow to fill in the holes in P. That creates a potential barrier, that prevents further electron flow, from N to P. Since at the barrier, N becomes positively charged and P becomes negatively charged, why aren't electrons flowing back? I think one way to answer the question is to answer the following: why do electrons even want to fill those holes?",1744484573.0,computerscience,STEM
1jxojcv,Whats computer science,"I'm watching the CS50 course for no obvious reason and am now in week 6 (Python), but to this point, I don't understand what ""CS"" means.",1744484136.0,computerscience,STEM
1jwujre,Best data structure for representing a partially ordered set (POSET) or lattices,"So I have recently been diving into refinement calculus because I found it to be really interesting and has potential for a lot of things, as I was going through the famous book , the chapter starts with a theoretical foundations on lattice theory, which forms the groundwork for later work. To further my understanding of them I wanted to implement them in code however iam not sure exactly what is the best way to represent them, since lattices are simply posets (partially ordered sets) but with extra conditions like bottom and top , I figured if I efficiently represent posets I can then extend the implementation to lattices, however even that seems to have so many different options, like adjacency matrix ,DAG (directed asyclic graphs), many other stuff. If anyone has any idea or can give me pointers on where I might find a cool resource for this I would be greatly appreciated. 

https://en.m.wikipedia.org/wiki/Lattice_(order)

https://en.m.wikipedia.org/wiki/Partially_ordered_set
",1744390134.0,computerscience,STEM
1jvx0g1,Low level programming as in actually doing it in binary lol,"I am not that much of a masochist so am doing it in assembly… anyone tried this bad boy?

https://www.ebay.com/itm/276666290370",1744289134.0,computerscience,STEM
1jvrj24,"If you had a non-deterministic computer, what would you do with it?","Brainstorming a writing idea and I thought I'd come here. Let's suppose, via supernatural/undefined means, someone is able to create a non-deterministic device that can be used for computation. Let's say it can take a function that accepts a number (of arbitrary size/precision) and return the first positive value for which that function returns true  (or return -1 if no such value exists). Suppose it runs in time equal to the the runtime of the worst case input (or maybe the run time of the first accepted output). Feel free to provide a better definition if you think of one or don't think mine works.

What (preferably non-obvious) problems would you try to solve with this?",1744267330.0,computerscience,STEM
1jvwjq5,How would I find a Minimum path cover in directed acyclic graph if the paths do not need to be vertex disjoint?,"I've found this Wikipedia article here, but I don't necessarily need the paths to be vertex disjoint for my purposes.

[https://en.wikipedia.org/wiki/Maximum\_flow\_problem#Minimum\_path\_cover\_in\_directed\_acyclic\_graph](https://en.wikipedia.org/wiki/Maximum_flow_problem#Minimum_path_cover_in_directed_acyclic_graph)

Is there some kind of modification I can make to this algorithm to allow for paths to share vertexes?

",1744287688.0,computerscience,STEM
1jvdxdi,I have come up with an algorithm doing set based topological sort.,"It performs topological sort on a directed acyclic graph, producing a linear sequence of sets of nodes in topological order.  The algorithm reveals structural parallelism in the graph.  Each set contains mutually independent nodes that can be used for parallel processing.

I've just finished the [algorithm] (https://github.com/williamw520/toposort/blob/master/Algorithm.md) write-up.

[Implementation](https://github.com/williamw520/toposort) was done in Zig, as I wanted to learn about Zig and it was an opportunity to do a deep dive.",1744225555.0,computerscience,STEM
1jviwhx,Cannot grasp some concepts from Charles Petzold’s Code,"Hey everybody, I've been reading Charles Petzold's book ""Code: The Hidden Language of Computer Hardware and Software"" 2nd edition and seemingly understood everything more or less. I'm now reading the chapter about memory and I can't seem to figure out some things:

1. There's this overview of how to build a 16x8 memory array efficiently. I can understand everything up to the second screenshot. It might be the wording or I stopped following Charles' train of thought at some point. My current understanding is this: the 4 to 16 decoder is used to generate a write signal for a concrete byte. Once generated, all data in values are stored within flip-flops (1st screenshot). Further, however, the author says that those end gates from the decoder are inputs to another set of end gates with another write signal. This is where I'm lost. What is that second write signal? Where does it come from? What's the point of it if the signal generated from the 4 to 16 decoder is seemingly enough to do that 0-1 clock transition and save the value in the flip-flop:

*Processing img wunmckic5gte1...*

*Processing img hlgdjr4k5gte1...*

2. Going further into the chapter, the author shows how we can read the value of a memory cell (the bits at a specific position in each byte are connected in columns). Then he says something I cannot understand, quote: ""At any time, only one of the 16 outputs of the 4-to-16 decoder will have an output of 1, which in reality is a voltage. **The rest will have an output of 0, indicating ground**"". I understand why 1 is voltage but why on earth does he refer to 0 as the ground? From what I understood having read this book for a long time is that the ground is basically a physical connection to the ground (earth) so that the circuit is closed without being visibly closed. Now he refers to the output of 0 as the ground and I'm completely confused. We cannot connect anything there to close the circuit, can we?

*Processing img i8efa2nd6gte1...*

3. And the last but not least, a little further the author says this: ""We could get rid of the giant OR gate if we could just connect all the outputs of the AND gates together. **But in general, directly connecting outputs of logic gates is not allowed because voltages might be connected directly to grounds, and that’s a short circuit.** But there is a way to do this using a transistor, like this:""

*Processing img hb36678i7gte1...*

And again I can't figure out where the ground is in that case and how connecting outputs of logic gates can cause short circuiting. Moreover, he also says this ""If the signal from the 4-to-16 decoder is 1, then the Data Out signal from the transistor emitter will be the same as the DO (Data Out) signal from the memory cell—either a voltage or a ground. **But if the signal from the 4-to-16 decoder is 0, then the transistor doesn’t let anything pass through, and the Data Out signal from the transistor emitter will be nothing—neither a voltage nor a ground.**"". What does this mean? How is nothing different from 0 if, from what I understood, 0 means no voltage and nothing basically also means no voltage?",1744238318.0,computerscience,STEM
1jv3db7,How do RAM and CPU work together?,"I want to understand better the concept of threads and functionality of RAM so please correct me if I am wrong. 

When u open an app the data, code and everything of that app gets stored in the ram to accessed quickly from there the threads in the cpu cores load up the data from the RAM which then then gets executed by the core and sent back to be displayed.",1744197763.0,computerscience,STEM
1jvfwf8,"On a historical scale, what was more important? Algorthm or Architecture?","
From an IT perspective, I’m wondering what has had the bigger long-term impact: the development of algorithms or the design of architectures.

Think of things like:
	•	Sorting algorithms vs. layered software architecture
	•	TCP/IP as a protocol stack vs. routing algorithms
	•	Clean Code principles vs. clever data structures
	•	Von Neumann architecture vs. Turing machine logic

Which has driven the industry more — clever logic or smart structure? Curious how others see this, especially with a view on software engineering, systems design, and historical impact.",1744230469.0,computerscience,STEM
1jvgowy,Perhaps every task is computational in nature?,"Define computation as a series of steps that grind the input to produce output. I would like to argue, then, that ""sing a song"" and ""add two and two"" are both computational. The difference is precision. The latter sounds more computational because with little effort, we can frame the problem such that a hypothetical machine can take us from the inputs (2 and 2) to the output (4). A Turing Machine, for example, can do this. The former seems less computational because it is vague. If one cares, they can recursively ""unpack"" the statement into a set of definitions that are increasingly unambiguous, define the characteristics of the solution, and describe an algorithm that may or may not halt when executed in a hypothetical machine (perhaps a bit more capable than TMs), but that does not affect the nature of the task, i.e., it's computability can still be argued; we just say no machine can compute it. Every such vague problem has an embedding into the space of computational tasks which can be arrived at by a similar ""unpacking"" procedure. This unpacking procedure itself is computational, but again, not necessarily deterministic in any machine.

Perhaps this is why defining what's a computational task is challenging? Because it inherently assumes that there even exist a classification of computational vs non-computational tasks.

As you can tell, this is all brain candy. I haven't concretely presented how to decompose ""sing a song"" and bring it to the level of precision where this computability I speak of can emerge. It's a bit arrogant to make any claims before I get there, but I am not making any claims here. I just want to get a taste of the counterarguments you can come up with for such a theory. Apologies if this feels like a waste of time.",1744232491.0,computerscience,STEM
1jv1l1l,A lot of algorithms in computer science or equations from maths are derived from physics or some other field of science.,"Many computer science algorithms or equations in math are derived from physics or some other field of science. The fact that something completely unrelated to the inspiration can lead to something so applicable is, first of all, cool asf.

I've heard about some math equations like the **brachistochrone curve**, which is the shortest path an object under gravity takes to go from one altitude to a lower one—it was derived by **Bernoulli using Snell's law**. Or how a few algorithms in **distributed computing** take inspiration from **Einstein's theory of relativity** (saw this in a video featuring **Leslie Lamport**).

Of course, there's the obvious one—**neural networks**, inspired by the structure of the brain. And from **chemistry**, we’ve got **simulated annealing** used for solving combinatorial optimization problems.

I guess what fascinates me the most is that these connections often weren’t even intentional—someone just noticed a pattern or behaviour in one domain that mapped beautifully onto a completely different problem. The creativity involved in making those leaps is... honestly, the only word that comes to mind is *cool*.

So here's a question for the community:  
**What are some other examples of computer science or math being inspired by concepts from physics, chemistry, biology, or any other field?**

Would love to hear some more of these cross-disciplinary connections.


**EDIT:** confused on the down votes (⁠ﾉﾟ⁠0ﾟ⁠)⁠ﾉ",1744190726.0,computerscience,STEM
1juf5zj,How (or do) game physics engines account for accumulated error?,"I've been playing around with making my own simple physics simulation (mainly to implement a force-directed graph drawing algorithm, so that I can create nicely placed tikz graphs. Also because it's fun). One thing that I've noticed is that accumulated error grows rather quickly. I was wondering if this ever comes up in non-scientific physics engines? Or is this ignored? ",1744123168.0,computerscience,STEM
1jtgdi7,How important is Linear Algebra?,"Ik it has applications in data analytics, neural networks and machine learning. It is hard, and I actually have learnt it before in uni but I couldn't see the real life applications and now I forgot everything 🤦🏻‍♂️",1744014474.0,computerscience,STEM
1js796v,Why do computers always have a single dimensional arrays. Why is memory single-dimensional? Are there any alternatives?,I feel this is to generalize so any kind of N dimensional space can be fit into the same one dimensional memory. but is there more to it?? Or is it just a design choice?,1743870324.0,computerscience,STEM
1jsvb8v,Is that true?,"Sparse Connections make the input such that a group of inputs connects to a specific neuron in the hidden layer if, for example, you know a specific domain. But if you don’t know that specific domain and you make it fully connected, meaning you connect all the inputs to the entire hidden layer, will the fully connected network then focus and try to achieve something like Sparse Connections can someone say that im right or not?",1743949569.0,computerscience,STEM
1jrgesk,How Well Does Bucketsort Work?,"Just to let you all know, my job is not in computer science, I am just someone who was curious after browsing Wikipedia. A sort takes an array or linked list and outputs a permutation of the same items but in order.

Bubble sort goes through the list, checks if one element is in order of the next one, and then swaps if they are out of order and repeats this until the array is in order.

Selection sort searches for the first element in the list, swaps it so that it occupies the first position, then looks for the second element, swaps it to the second position, looks for the third element, swaps it to the third position, and so on.

Insertion sort I don't really know how to explain well. But it seems to be ""growing"" a sorted list by inserting elements. If the next element is larger than the end of the list you are inserting, you add it to the end, if not, keep swapping until it ends up in the right place. So one side has an already sorted list as the sort is fed unsorted items, It is useful for nearly sorted lists. So I guess if you have a list of 10 million items and you know at most 3,000 are not in their right place, this is great since less than 1/1000 items are out of place.

Stooge sort is a ""joke impractical"" sort that made me laugh. I wonder if you can make a sort with an average case of N\^K with K being whatever integer above 2 you want but a best case of O(N).

Quicksort is kind of a divide and conquer. Pick a pivot point, then put everything below the pivot on one side and everything else on the other side, then do it again on each sublist I guess this is great parallel processing, but apparently this is better than Insertion sort even with serial processing.

Bucket sort puts items in buckets and then does a ""real sort"" within each bucket. So I guess you could have a 0 to 1000 bucket, a 1001 to 2000, a 2001 to 3000 and a above 3001 for 4 buckets. This would be very bad if we had 999 items below 1000 and each other bucket had 1 item in it.

Assuming some uniformity in data, how well does Bucket sort compare to quicksort? Say we had 130 buckets, and we were reasonably sure there would be an average of 10 items, we'll say are integers, in each Bucket 3 at a minimum. I'm not even sure how we choose our bucket size. If we commit to 130 buckets and knew our largest integer was 130,000, then each bucket can be 1,000 size. But if you tell your program ""here is a list, sort them into 130 buckets, then do a comparison sort on each bucket"" it would need to find the largest integer. To do that, it would have to go through the entire list. And if it needed to find the largest integer, it could have just done quicksort and start sorting the list without spending time to find the largest one.",1743784847.0,computerscience,STEM
1jqgyzj,Was there ever a time where it was widespread to erroneously use kibibyte to mean 1000bytes?,"I'm a bit flabbergasted right now and this is genuinely embarrassing. I have a software engineering masters degree from a top university that I graduated from about 20 years ago - and while my memory is admittedly shit, I could have sworn that we learned a kilobyte to be 1024 bytes and a kibibyte to mean 1000bytes - and now I see it's actually the other way around?
Is my brain just this fucked or was there a time where these two terms were applied the other way around?",1743682880.0,computerscience,STEM
1jpydd0,co-nondeterministic Turing Machines,"https://preview.redd.it/dk2tah0j9hse1.png?width=3594&format=png&auto=webp&s=7eaf502ea0b321b0249cc2c7ea2cdeedf354b5e5

Hello,   
  
so I have an exam coming up and this was one of the question from a previous exam.  
  
A simple Turing Machine which we could quickly realize what L\_N in this case is: { w | w ∈ {a, b}\* and |w| >= 2 }.  But when it comes to L\_coN, the language where M behaves as a co-nondeterministic TM, what would the language be? Sure I understand that a coNTM must evaluate every path it takes to true (it accepts) otherwise it would reject, but what does it exactly mean in this context?

And for some reason there is no information about such TMs on the the internet, any help would be greatly appreciated! 

Thank you.",1743624344.0,computerscience,STEM
1jplqj1,Counting from 0,"When did this become a thing?

Just curious because, surprisingly, it's apparently still up for debate",1743591423.0,computerscience,STEM
1jnz0aq,"[blog] if you want to browse the internet, you must first invent the universe",,1743412401.0,computerscience,STEM
1jngz27,"What exactly is a ""buffer""","I had some very simple C code:

```clang
int main() {
  while (1) {
    prompt_choice();
  }
}

void prompt_choice() {
  printf(""Enter your choice: "");
  int choice;
  scanf(""%d"", &choice);
  switch (choice) {
    case 1:
      /* create_binary_file(); */
      printf(""your choice %d"", choice);
      break;
    default:
      printf(""Invalid choice. Please try again.\n"");
  }
}
```

I was playing around with different inputs, and tried out `A` instead of some valid inputs and I found my program infinite looping. When I input `A`, the buffer for `scanf` doesn't clear and so that's why we keep hitting the default condition.

So I understand to some extent why this is infinite looping, but what I don't really understand is this concept of a ""buffer"". It's referenced a lot more in low-level programming than in higher level languges (e.g., Ruby). So from a computer science perspective, what is a buffer? How can I build a mental model around them, and what are their limitations?",1743354061.0,computerscience,STEM
1jmqb1l,Inside arXiv—the Most Transformative Platform in All of Science,Really cool article about the people behind something we all take for granted.,1743266292.0,computerscience,STEM
1jmawcn,Leading research for consensus mechanisms?,"What are the current innovations in this area of study? I'm really interested about the ""cutting edge"" of this, if there's anything like that going on. I feel like a greater emphasis on the efficiency of cryptographic mining will be happening sooner than later, and consensus algorithms will become a prime means of reducing resource use. Any references/dissertations/articles would be appreciated!",1743210695.0,computerscience,STEM
1jlrumb,"ask network guys, why upload speed tends to be much slower than download speed?","here, ""speed"" refers to casual, daily-life meaning.

an example is when we upload/download a file(s) to/from a cloud storage service. speed gap is obvious.

I'm not sure but I suspect that one of the reasons is that the server performs safety check on files which will be uploaded on. And this might be enough, but I wonder if there are further reasons.",1743157254.0,computerscience,STEM
1jlnno8,How do I make programs that are more friendly to the system in terms of performance? Is it worth even trying?,"This isn’t a question about algorithmic optimization. I’m curious about how in a modern practical system with an operating system, can I structure my code to simply execute faster. I’m familiar with some low level concepts that tie into performance such as caching, scheduling, paging/swapping, etc. . I understand the impact these have on performance, but are there ways I can leverage them to make my software faster? I hear a lot about programs being “cache friendly.” Does this just mean maintaining a relatively small memory footprint and accessing close by memory chunks more often? Does having immutable data effect this by causing fewer cache invalidations? Are there ways of spacing out CPU and IO bound operations in such a way as to be more beneficial for my process in the eyes of the scheduler? In practice, if these are possible, how would you actually accomplish this in code? Another question I think it worth the discussion, the people who made the operating system are probably much smarter than me. It’s likely that they know better. Should I just stay out of the way and not try to interfere? Would my programs be better off just behaving like any other average program so it can be more predictable? (E to add: I would think this applies to compiler optimizations as well. Where is it worth drawing the line of letting the optimizations do their thing? By going overboard w hand written optimizations, could I be creating less common patterns that the compiler may not be made to optimize as well?) I would assume most discussion around this would also apply mostly to lower level languages like C which I’m fine with. Most code I write these days is C and Rust with some Python for work. 

If you’re curious, I’m particularly interested in this topic for a personal project to develop a solver for [nonagrams](https://en.m.wikipedia.org/wiki/Nonogram). I’m using this as a personal challenge to learn about optimization at all levels. I really want to just push the limits of my skills and optimization. My current, somewhat basic, implementation is written in rust, but I’m planning on rewriting parts in C as I go. ",1743138623.0,computerscience,STEM
1jlk2d7,"not exactly sure if this fits here, but in this building game i like i made a very basic binary computer :D (im not good at computer science i plan to go into the medical field)","basically that REPEATER gate is always active which triggers one part of the AND gate, which that gate's other input is a lever. that triggers an actual repeating REPEATER goes into a DELAY which turns on the binary value ""1,"" and that also triggers an INVERTER, so when that DELAY is off the INVERTER triggers the ""0"" light. do yall think i did good? first time doing anything like this ",1743126699.0,computerscience,STEM
1jkir4v,What are some papers/ thesus/ books every programmer should read,,1743013409.0,computerscience,STEM
1jkf9wi,"any studies, researches, etc. on AVERAGE-case maximization in adversarial game tree search?","for example, in chess programming, all contemporary competitive engines are heavily depending on minimax search, a worst-case maximization approach.

basically, all advanced search optimization techniques(see chess programming wiki if you have interests, though off-topic) are extremely based on the minimax assumption.

but due to academic curiosity, i'm beginning to wonder and trying experiment other approaches. average maximization is one of those. i won't apply it for chess, but other games.

tbh, there are at least 2 reasons for this. one is that the average maximizer could outperform the worst maximizer against an opponent who doesn't play optimally.(not to be confused with direct match of both two)

the other is that in stochastic games where probabilistic nature is involved, the average maximizer makes more sense.

unfortunately, it looks like traditional sound pruning techniques(like alpha-beta) are making no sense anymore at the moment. so i need help from you guys.

if my question is ambiguous, please let me know.

thanks in advance.",1743004911.0,computerscience,STEM
1jkqn5m,Is this a mistake in CODE?,"Is this another mistake in CODE by Charles Petzold? I’m confused?

In the first picture we have the register array. As you can see, the “Select Input” bits go into the CLOCK inputs of the latches. So these “Select Input” bits correspond to the latch that’s about to have Data “In” written into it.

The “Select Output” correspond to the TRI enable for each latch, so these bits select which register is having its data put on the data bus.

In the second page we have the general form for some instruction codes.

Consider the instruction MOV r,r. 
This instruction moves a byte from a source register (SSS) to a destination register (DDD) within the same registry array.

e.g if you look at the table on the second picture, you could infer that the instruction byte for MOV B,C would

01000001

HERE'S WHERE I'M CONFUSED

Look at the diagram for
 ""Instruction Latch 1: Opcode"" on the third page I’ve added.

You can see that C5C4C3 go into 
RA OUTPUT select (RA being register array)

And you can see that C2C1C0 (SSS) go into
RA INPUT Select 

Look at the picture of the RA in the first page; surely it should be the other way round? 

If the 3 rightmost bits are the source register, then surely we want to output the byte at this register?

e.g for 01000001 (MOV B,C) we’d have the contents of C assigned to B
B <- C 

would we not want to route the 001 (Register C, the Source) to RA Output Select? And then route the 000 (Register B, the destination) to RA Input select? Page 3 implies 01SSSDDD for the general form, when it should be 01DDDSSS

Hopefully I've explained this clearly. If not I can elaborate.",1743033461.0,computerscience,STEM
1jk7aup,Game theory problem?,"Imagine an oracle that takes in a Turing machine as input. The oracle has inside of it a correct response function that outputs the input machines run length if it halts, or infinity if it never halts, and an incorrect response function that outputs whatever it can to ensure the oracle gives as little information as possible about the set of all Turing machine outputs. The incorrect response function is able to simulate the oracle, and the correct response function. For every unique input, the oracle randomly decides with a 50/50 chance which functions output to output, and the oracle will always output the same output for a given input.
What information, if any, could be gained from this? What would some of the behaviors of the incorrect response function be? Could an actual oracle be created from this?

(Sorry if this is a poorly structured question)",1742979168.0,computerscience,STEM
1jj5dod,What are active areas of TCS that are not ML-related?,"It feels like often when I see a talk at a theory seminar or read a prof's research interests, it is often something along the lines of ""My research lies at the intersection between theoretical computer science and machine learning."" My question is what are the most active parts of TCS that are not at the intersection with ML?",1742858924.0,computerscience,STEM
1jiyqpp,Difference between throughput and transfer rate,What is the difference between throughput and transfer rate in terms of sending a file over a network? I’m a bit confused as the terms seem to be the same to me lol. I need to do some experiments where I measure each of them but I’m struggling with what I’m actually measuring for each of them.,1742842807.0,computerscience,STEM
1ji5oea,Help Please! Quadtrees Complexity!,"Hello!

I am working on terrain and, long story short, the method I am trying to follow to split it up in levels of details involves quadtrees. I did some digging into the complexity of classic operations (such as adding/removing/retrieving information) with quadtrees and it would seem that it is generally logarithmic. I wanted to find a somewhat detailed proof to understand how the sources I found get to that result, but I couldn't (there also seems to be slightly varying information between it being O(lnN) or O(log2(N)).

When I try to figure out the proof on my own (I never really studied CS, so complexity demonstrations are far from my forte) I seem to find something closer to linear complexity (which, if I've understood correctly, would kind of defeat the purpose of using a quadtree since it's the same complexity as a list). One of my proof attempts resulted in constant complexity so I'm obviously making mistakes.

I know this might be asking a lot, but could someone walk me through it please?

Thanks in advance!

PS: apologies if I misused math/CS terms, English isn't my first language",1742753591.0,computerscience,STEM
1jipxes,Initial Draft Paper: N ~bijects R,"https://www.overleaf.com/read/jhmvjvtdntcc#be15b6

The overall concept is simple and presented clearly. What should I refine? I can add code, the implementation is actually very simple, and I can do it trivially in hardware as well.

There are some visual results of applying the algorithm on my X post: https://x.com/alegator_cs/status/1904142557572894789",1742820758.0,computerscience,STEM
1jhx2c6,Book for Parallel computing,I feel like I really need a book for parallel computing course. Is there any recommendation simply explained parallel computing?,1742728910.0,computerscience,STEM
1jhv4lh,Language Specialized for Parallel Sorts,"I’ve been exploring multithreading and parallel sorting methodologies through Java and was wondering if there is a language specialized for this type of computation. Also, is it possible to optimize by abusing the JVM specifically PC Registers in the JVM Memory Areas or does it already do something of the sorts (I am confused about the nuances of how the JVM works so if you could refer me to a place where i can learn that’d be nice) ",1742720585.0,computerscience,STEM
1jh8afp,City walking algorithm,"**NOTE: This is not a homework assignment, rather a business problem we are trying to solve for a maintenance contract in the downtown core of a major city.** 

Given a square grid/map of the downtown of a modern city, what is the most efficient way to walk the entire surface area of the sidewalks (two on each street, north/south and east/west) with the least amount of overlap and in the least amount of time?

Assumptions:

* a constant speed is assumed
* 4 people are available to do the walking",1742649646.0,computerscience,STEM
1ji9m03,Someone just created this new fastest sorting algorithm. “Recombinant Sort” What do you guys think about it ? And why is it not famous ?,,1742763571.0,computerscience,STEM
1jh4p5j,Graph which complementer also has exponential shortest paths,"Let’s say we have undirected unweighted discrete graph without self-loops. I found that enumerating all shortest paths between each pair of nodes could be super-exponential in input size. 

Is it possible to construct such graph with exponential shortest paths, that its complementer also has exponential shortest paths count?",1742635784.0,computerscience,STEM
1jggpq7,How do I get started with writing an research paper & find people to collaborate with?,"Hey guys,
I want to write an ML research paper but have no idea where to start. I’ve worked on deep learning stuff and done NLP projects like sentiment analysis,implementing research papers, fine tuning etc  but never written a proper paper before.How do I get started?
Where do people usually find collaborators or Mentors for this?
If anyone has experience with this or wants to team up, hit me up! Would love to get some guidance.",1742563399.0,computerscience,STEM
1jfv1tk,New prime algorithm I just made,"Hi, I just published a research paper about a new prime generation algorithm that's alot more memory efficient than the sieve of Eratosthenes, and is faster at bigger numbers from some tests I made. Here's the link to the paper : https://doi.org/10.5281/zenodo.15055003 there's also a github link with the open-source python code, what do you think?",1742493389.0,computerscience,STEM
1jgcwws,Path-finding on a grid with multiple source-destination pairs and non-crossing paths,"Hello! This is very similar to a 2-year-old post here, but the OP [didn't get an applicable answer](https://www.reddit.com/r/computerscience/comments/115omu4/comment/lma9sgc/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button), so I will post my question here too.

There is an infinite 2D square grid, every cell of which can be either empty or occupied by a wall. A path is defined by a sequence of moves either by 1 or 2 cells straight or by 1 cell in a diagonal direction. Given an array of source-destination vertex pairs, is it possible to find (shortest in total) paths that don't cross each other?

I've looked into some path-finding algorithms like A\*, but that didn't work for me. My current approach is to do subsequent searches while marking cells, walked by each path as walls. However, this isn't great, even if I sort the vertex pairs by distance, because sometimes my algoritm can't find a solution even if there is. I've also searched for disjoint paths on grid, but I couldn't find an algoritm suitable for my case as paths can 'jump' by two cells.

P.S. I need this algorithm for a mod of a very unpopular circuit-creating game.

P.P.S. I found some scientific works but I'm too stupid to understand them :(, it would be very nice if there is an example implementation in some programming language.

Thanks in advance!",1742549623.0,computerscience,STEM
1jg8n35,Why Enterprise service busline or ESB?,"Why do we need ESB? 

What's the point of it? 

Why does it exist? ",1742531087.0,computerscience,STEM
1jfjirz,Is this a mistake in this textbook?,"This example looks more like n^2 than n log n

Foundations of computer science - Behrouz Forouzan",1742456093.0,computerscience,STEM
1jfh8pj,funny thought,I downloaded wireshark today(night) for a networking and security assignment I have due soon and im finally seeing what my internet does. anyone else find themselves wondering just how many of these captured 'wires' are malware packets sending back information to their creator because you downloaded a certain modded mobile app game on a sketchy sight over a year ago,1742446224.0,computerscience,STEM
1jewm2c,How does Wires Computing effect your daily use?,"I'm writing an essay for a class and need some users input. The premise is about how Wires effect users and their computing. As in the more we use our devices, such as cell phones, computers, tablets etc. the more we desire everything to be wireless. So when we get a computer that has less ports for example and everything is wireless, such as bluetooth, wifi, wireless hdmi. Does that make the experience better because we need less to do what we want? Or does it make it worse because we feel less in control of the device we're using because we can't simply plug what we need into the unit for it to work?

>Think hdmi for example, you want to hook something to your TV, and hdmi cable is great and a simple solution, we're 100% in control. Most devices have wireless casting built-in now, which can work, but we have to ensure we're on the same network, all the settings are proper etc.

Each has it's pros and cons, have we gotten to the point where we just deal with things, or do we still seek out computers (laptops, tablets) that have more to give us control

So as in the first question... How do your wires effect your computing?

*\*\*Meant to title it ""How do your wires effect your computing?""*",1742389855.0,computerscience,STEM
1jeeb97,Why do games use udp instead of tcp?,"I’m learning computer networks right now in school and i’ve learned online games use udp instead of tcp but i don’t really understand why? I understand udp transmits packets faster which I can see being valuable in online games that are constantly updating, but no congestion or flow control or rdt seems like too big of a drawback in them too. Wouldn’t it be better to ensure every packet is accurate in competitive games for accuracy or is udp that much faster that it doesn’t matter? Also, would congestion and flow control help when servers experience a lot of traffic and help prevent lagging and crashing or would it just make it worse?",1742328013.0,computerscience,STEM
1jenl3g,How would a Pentium 4 computer perform with today's fabrication technology?,"The [Pentium 4](https://en.wikipedia.org/wiki/Pentium_4) processor was launched in 2000, and is one of the last mainstream 32-bit architectures to feature a single core.  It was fabricated using a 130 nm process, and one of the models had a 217 mm^2 die size.  The frequency varied up to 3.8 Ghz, and it could do 12 GFLOP/s.

Nowadays though, we can make chips on a 2 nm process, so it stands to reason that we could do a massive die shrink and get a teeny tiny pentium 4 with much better specs.  I know that the process scale is more complicated than it looks, and a 50 nm chip isn't necessarily a quarter of the size of a die-shrunk 100 nm chip. _But_, if it did work like that, a 2 nm die shrink would be 0.05 mm^2 instead of 217.  You could fit over 4200 copies on the original die.  GPU's do something similar, suggesting that one could have a gpu where each shader core has the power of a full-fledged pentium 4.  Maybe they already do? 12 GFlops times 4200 cores suggests a 50 TFlop chip.  Contrast this with the 104 TFlops of a RTX 5090, which is triple the die size, and it looks competitive.  OTOH, the 5090 uses a 5nm process, not 2; so the 5090 still ends up having 67% more flops per mm even after adjusting for density.  But from what I understand, their cores are much simpler, share L1/2, and they aren't going to provide the bells and whistles of a full CPU, including hundreds of instructions, pipelining, extra registers, stacks, etc.

But back to the 'Pentium 4 nano'.  So you'd end up with a die that's maybe 64 mm^2, and somewhere in the middle is a tiny 0.2x0.2 mm copy of the pentium 4 processor.  Most of the chip is dedicated to interlinks and bond wire, since you need to get the IO fed to a 478 pin package.  If the interlinks are around the perimeter of the CPU itself, they'd have to be spaced about 2 micrometers apart.  The tiny chip would make a negligible amount of heat and take tiny amounts of energy to run.  It wouldn't even need a cpu cooler anymore, as it could be passively cooled due to how big any practical die would be compared to the chip image.  Instead of using 100 watts, it ought to need on the order of 20 milliwatts instead, which is like 0.25% of an led.  There's losses and inefficiencies, things that have a minimal current to activate and stuff, but the point is that the CPU would go from half of the energy use of the system to something akin to a random pull-up resistor.

So far I'm assuming the new system is still running at the 3.8 Ghz peak.  But since it isn't generating much heat anymore (the main bottleneck), it could be overclocked dramatically.  You aren't going to get multiple terahertz or anything, but considering that the overclock record is [7.1 Ghz](https://forums.digitalspy.com/discussion/269075/pentium-4-overclocked-to-7-1-ghz-ouch), mostly limited by thermals, it should be easy to beat.  Maybe 12 Ghz out of the box without special considerations.  But with the heat problem being solved, you run into other issues like the speed of light.  At 12 ghz, a signal can only move about 9 inches per cycle.  So the ram needs to be less than four inches away for some instructions, round-trip times to the north/south bridge becomes an issue, response times from the bus/ram and peripheral components, there's latency problems like hysteresis from having to dis/charge the mass of a connection wire to transmit a signal, and probably a bunch of other stuff I haven't thought of.

  A workaround is to move components from the motherboard onto the same chip as the CPU.  Intel et al did this a decade ago when they eliminated the north bridge, and they moved the gpu onto the die for mobile (also allowing it to act as a co-processor for video and stuff).  There's also the added bonus of not needing the 471 pin cpu socket, and just running the traces directly to their destinations.  It seems plausible to make a chip that has our nano Pentium 4 on it, the maximum 4 Gb of ram, north bridge, GeForce 4 graphics card, AGP bus, and maybe some other auxiliary components all onto a single little chip.  Perhaps even emulate an 80Gb harddrive off in the corner somewhere.  By getting as much of the hardware onto a single chip as possible, the round-trip distance plummets by an order of magnitude or two allowing for at least 50-200 Ghz clock speeds.  multiple Terahertz is still out [due to Heisenberg](https://www.electropages.com/blog/2022/04/scientists-determine-maximum-theoretical-limit-optoelectronic-devices), but you could still make an early-2000's style desktop computer at least 50 times faster than what was, using period hardware designs.  And the whole motherboard would be smaller than a credit card.

Well, that's my 15 year old idea, any thoughts?  I'm uncertain about the peak performance, particularly things like how hard it would be to generate a clean clock signal at those speeds, or how the original design deals with new race conditions and timing issues.  I also don't know how die shrinks affect TDP, just that smaller means less heat and lower voltages.  Half the surface area might mean half the heat, a quarter, or maybe something weird like T^4 or log.  CD-roms would be a problem (80 pin IDE anyone?), although you could still install windows over a network with the right bios.  The PSU could be much smaller and simpler, and the lower power draw would allow for things like using buck converters instead of large capacitors and other passives.  I'd permit sneaking other new technologies in, just as long as the cpu architecture is constant and the OS can't tell the difference.  Less cooling and wasted space imply that space savings could be had elsewhere, so instead of a big Dell tower, the thing could be a TiTac box with some usb ports and a VGA.  It should be possible to run the video output through usb3 instead of the vga too, but I'm not sure how well AGP would handle it since it predates HDMI by several years.  Maybe just add a vga-usb converter on die to make it a moot point, or maybe they have the same analog pin anyway? P4 was also around the time they were switching to pci express, so while mobos existed with either interface, the AGP comes with extra hurdles with how ram is utilized, and this may cause subtle issues with the overclocking.

The system on a chip idea isn't new, but the principle could be applied to miniaturize other things like vintage game consoles.  Anything you might add on that could be fun; my old PSP can run playstation and N64 games despite being 30x smaller and including extra hardware like screen, battery, controls, etc.",1742353565.0,computerscience,STEM
1jef1en,Why do IPv4 and IPv6 use constant length addresses?,"Why is this preferable to say, an organization that simply has a terminator to the address. (Like null terminated strings.)  
Such an organization could be (altho marginally) more efficient, since addresses that take less bytes would be faster and simpler to transmit. It would also effectively never run out of address space. (avoiding the problem we ran into with IPv4- altho yes, I know IPv6 supports an astronomically high number of addresses, so this realistically will never again be a problem.)  
  
I ask because I'm developing my own internet system in Minecraft, and this has been deemed preferable in that context. My telecommunications teacher could not answer this, and from his point of view such a system is also preferable. Is there something I'm missing?",1742329754.0,computerscience,STEM
1jel183,In python why is // used in path while / is used elsewhere?,Could not find the answer online so decided to ask here.,1742345641.0,computerscience,STEM
1jd364i,Resources for understanding / building ATP (Automated theorem proving),,1742180831.0,computerscience,STEM
1jcp5jx,Automata Theory NFA to DFA?,"I'm looking at NFA to DFA conversion through subset constriction. In the book I'm reading I believe it shows the {q1,q2} as a DFA state but looking above it I can't see any single transition that leads to both of those states? Can someone explain why it's on there? q2 has not outgoing transitions so I can't see any reason for it to be a DFA state?",1742142101.0,computerscience,STEM
1jcq1w7,How to define an algorithm for generating a check digit without access to the source code?,"I'm stuck on a problem and hoping some of you brilliant minds can offer some guidance. I'm trying to figure out the algorithm used to generate the check digit (the last digit) of a 16-digit ID. I don't have access to the source code or any documentation, so I'm trying to reverse engineer it.

Here's what I know about the **ID structure**:

* XXX-XX-XXXXXXXXXX-Y
* XXX: Country code.
* XX: Last two digits of the year (e.g., ""22"", ""23"").
* XXXXXXXXXX: A 10-digit sequential number, padded with leading zeros.
* Y: The check digit (0-9).

**Real Examples**: 6432300045512011, 6432300045512028, 6432300045512030, 6432300045512049, 6432300045512053, 6432300045512066

**My Goal**: Determine the algorithm used to calculate Y (the check digit).

What I've Tried *(and Why it Failed)*:

I have a dataset of millions of these IDs. I've approached this from several angles, but I'm hitting a wall:

1. **Statistical Analysis**:

* **Check Digit Distribution**: The check digits (0-9) are roughly evenly distributed. A histogram shows no obvious bias.
* **Correlation Analysis** (Pearson, Spearman, Kendall): Extremely low correlation (< 0.001) between the check digit and any other individual digit or combination of digits. A heatmap confirms this – virtually no correlation.
* **Modulo Analysis**: I tested taking the sum of the first 15 digits modulo n (where n ranged from 6 to 12). The remainders were uniformly distributed, especially for moduli 10 and 11. This suggests a modulo operation might be involved, but it's not straightforward.
* **Regression Analysis**: Linear regression models performed very poorly, indicating a non-linear relationship.
* **Difference Analysis**: I examined the differences between consecutive IDs and their corresponding check digits. The IDs are mostly sequential (incrementing by 1). However, the change in the check digit is unpredictable, even with a small change in the ID.

**Conclusion from Statistical Analysis**: The algorithm is likely good at ""mixing"" the input. There's no simple linear relationship. The sequential nature of the IDs, combined with the unpredictable check digit changes, is a key observation.

2. **Genetic Algorithm**:

**Approach**: I tried to evolve a set of weights (one for each of the first 15 digits) and a modulus, aiming to minimize the error between the calculated check digit and the actual check digit.

**Result**: The algorithm quickly stagnated, achieving only around 10% accuracy (basically random guessing).

3. **Known Algorithms**:

I tested common checksum algorithms (Luhn, CRC, ISBN, EAN) and hash functions (MD5, SHA-1, SHA-256). None of them matched.

4. **Brute-Force (Simulated Annealing)**:

Tried a simulated annealing approach to explore the vast search space of possible weights and operations.

**Result**: Computationally infeasible due to the sheer number of combinations, especially given the strong evidence of non-linearity.

5. **Neural network**

**Architecture**: Simple fully connected network (15 inputs → hidden layers → 1 output).

Since I am not an expert in machine learning, the neural network predictably failed to produce any results. The learning progress stopped quickly and halted at **10% accuracy**, which corresponds to complete randomness.

The algorithm likely involves non-linear operations before or after the weighted sum (or instead of it entirely). Possibilities include:

* Perhaps bitwise operations (XOR, shifts, etc.) are involved, given the seemingly random nature of the check digit changes.
* Something more complex than a simple sum % modulus might be happening.
* Each digit might be transformed by a function (e.g., exponentiation, logarithm, lookup table) before being weighted.

**My Questions for the Community**:

1. Beyond what I've tried, what other techniques could I use to analyze this type of check digit algorithm? I'm particularly interested in methods that can handle non-linear relationships.
2. Are there any less common checksum or cryptographic algorithms that I should investigate? I'm looking for anything that might produce this kind of ""well-mixed"" output.
3. Could Neural Networks be a viable approach here? If so, what kind of architecture and training data would be most effective? I'm thinking about using a sequence-to-one model (inputting the first 15 digits, predicting the 16th). What are the potential pitfalls?
4. Is it make sense to try to find collisions, when two diffrent numbers produce the same control number?

I'm really eager to hear your ideas and suggestions. Thanks in advance for your help!",1742144456.0,computerscience,STEM
1jcfwvy,Self-study roadmap for Quantum Computing,"Prerequisites:
- linear algebra (vectors, matrices, eigenvalues, tensor products)
- complex numbers
- if you know the basics of quantum mechanics then well done
- calculus 
- Probability theory (i would recommend it for quantum algorithms & information theory)

Basics:
1) For interactive intro: https://quantum.country/qcvc
2) Old is gold yk so go through this playlist:
https://www.youtube.com/watch?v=F_Riqjdh2oM&list=PL1826E60FD05B44E4
3) For quantum circuit & gates:
https://qiskit.org/textbook/
4) To run simple simple quantum programs:
https://quantum-computing.ibm.com/

Intermediate:
Welcome homie
1) Principles of Quantum Computation and Information - Volume I then II
2) Quantum algorithms - https://qiskit.org/textbook/ch-algorithms/
3) For physics part:
https://www.youtube.com/watch?v=w08pSFsAZvE&list=PL0ojjrEqIyPy-1RRD8cTD_lF1hflo89Iu
4) Practice coding quantum algorithms using Qiskit or Cirq
https://quantumai.google/cirq/tutorials 

Advance level:
I myself not aware of much here but if you wanna explore research oriented side and theoretical knowledge then i know some books.
1) Quantum Computation and Quantum Information by Nielsen & Chuang
2) An Introduction to Quantum Computing by Kaye, Laflamme & Mosca
3) IBM Quantum Experience and Amazon Braket https://aws.amazon.com/braket/ for cloud-based quantum computing.


Quantum computing is vast so learning it in a month or day (humph not possible) you can also learn quantum complexity theory but this is focused on practical quantum computing.",1742108766.0,computerscience,STEM
1jdc79p,We're teaching Computer Science like it's 1999!!,"FACT: 65% of today's elementary students will work in jobs that don't exist yet.

But we're teaching Computer Science like it's 1999. 📊😳

Current computer science education:

• First code at age 18+ (too late!)

• Heavy theory, light application

• Linear algebra without context

My proposal:

• Coding basics by age 10

• Computational thinking across subjects

• Applied math with immediate relevance

Who believes our children deserve education designed for their future, not our past?",1742216841.0,computerscience,STEM
1jcna30,Address bus and for bits.,"I have been hassling you nice people about the way an address bus works with bits being placed on the rails, and how that happens. I think the orientation of the process has confused me! I have a book on the COMPTIA A+, and there is a pic of the RAM being put on the address bus, but it is twisted at 90 degrees, so you see the individuals bit’s going across the bus. But is they show it like that, then I see the number of bits as in more like an X axis (almost), rather than the number of bits being more like a Y axis. So know how the MCC gets stuff and how it places it on the rails is the tricky bit. Is it like a X horizontal axis going across the bus rails, or like a Y vertical axis.

That being the case, it’s important to know when the MCC gives and address for a certain bit of memory, how that address is requested. For example - line (or rail 4), and then depending on the number of BITS the system is, the MCC takes the X number of BITS and put it On the rails. I assume it take all that row of bits (although there would be no point having more bits to start with.

This diagram helped me a bit.

[http://www.cs.emory.edu/\~cheung/Courses/561/Syllabus/1-Intro/1-Comp-Arch/memory.html](http://www.cs.emory.edu/~cheung/Courses/561/Syllabus/1-Intro/1-Comp-Arch/memory.html)",1742137057.0,computerscience,STEM
1jc4klh,As We May Think (1945),,1742071092.0,computerscience,STEM
1jblzw6,How do you create a new programming language?,"Hey, inexperienced cs student here. How does one create a new programming language? Don't you need an existing programming language to create a new programming language? How was the first programming language created?",1742008588.0,computerscience,STEM
1jc79kw,Do this look bad?,"I made this 8 bit adder and ik it looks messy, but i wanted to know if its way too messy.

if so, how do i made it look better?

Btw, i also wanted to know if theres smth wrong on my desing. i mean, it works, but maybe theres smth that didnt needed to be there, or smth that should be there at all.

https://preview.redd.it/njye5qmjmxoe1.png?width=1920&format=png&auto=webp&s=92427feebed45cc2822e6e840b90c50541fb6bbb

",1742078886.0,computerscience,STEM
1jbv5rl,Memory bandwidth vs clock speed,"I was wondering, 

What type of process are more subject to take advantage of high memory bandwidth speed (and multi threading) ?

And what type of process typically benefits from cores having high clock speed ?

And if there is one of them to prioritize in a system, which one would it be and why ?

Thanks !",1742045457.0,computerscience,STEM
1jc8iu6,No Chance of Creating Something Like .NET on my Own,"I have long wanted to create my own programming language. A long time I have wanted, not only to create my own programming language, but to create an entire virtual machine like the CLR, and an entire framework like .NET. However, I face two obstacles in pursuing this, one, that I understand little about compilation, virtual machines, machine language, etc, and two, that such tasks require large teams of people and many hours of work to accomplish. Though it may seem that more easily, I might succeed at overcoming the first obstacle, there is much to learn about even the basics of compilers, from what I understand. And I can hardly withstand the urge to give up reading books on these topics while attempting to read the first chapter, fully understanding and retaining the information contained in it. Therefore I ask: Can I still create something like .NET?",1742082562.0,computerscience,STEM
1jb0upr,I found this book while searching for something related to Algorithms,"Hey guys
I found this book in my closet 
I never knew I had this
Can this book be useful?
It says 3d visualisation 
So what should I know in order to get to know the contents of this?",1741948475.0,computerscience,STEM
1jblbey,SHA1 Text collisions,"are there any known sha1 text collisions? i know there's google's shattered io and this research paper([https://eprint.iacr.org/2020/014.pdf](https://eprint.iacr.org/2020/014.pdf)), but im pretty sure both of those are binary files. Other than those two, are there any text collisions? like something i could paste into a text box.",1742006355.0,computerscience,STEM
1jbjwqs,Confused about exam question,"Hi, I recently took the 2023 OCR a level paper 1 and was confused about this question, couldn't you draw a box along the top of the diagram? why not?

https://preview.redd.it/3l04eh10aroe1.png?width=685&format=png&auto=webp&s=77f89ab947132e9400d5356bd89dd3de100fb6c3

",1742001904.0,computerscience,STEM
1jaoyu1,Etymology of Cookies.,"I was explaining what cookies actually ARE to my roommate. She asked why the name and I was stu.oed. of course Wikipedia has all the I fo on all the different kinds and functions but the origin of the name literally says it is a reference to ""Magic cookies"" sometimes just called Cookies. And the article for *that* doesn't address why tf THOSE were named cookies. 


Anybody know the background history on this?

Until I learn some actual facts im just gonna tell people that they are called cookies because magic internet goblins leave crumbs in your computer whenever you visit their websites. ",1741905721.0,computerscience,STEM
1jaozn4,Graph theory and its application,"Graph theory in real world applications

I've been interested lately in graph theory, I found it fun but my issue is that I can't really formulate real world applications into graph theory problems. I would pick a problem X that I might think it can be formulated as a graph problem, If I make the problem X so simple it works but as soon as I add some constraints i can't find a way to represent the problem X as a graph problem that is fundamental in graph theory.. I want to use the fundamental graph theories to resolve real world problems.
I am no expert on the field so it might be that it's just a skill issue",1741905780.0,computerscience,STEM
1jafwbw,AMA with Stanford CS professor and co-founder of Code in Place today @ 12pm PT,"Hi r/computerscience, Chris Piech, a CS professor at Stanford University and lead of the free Code in Place program here at Stanford is doing an AMA today 12pm PT, and would love to answer your Qs!

He will be answering Qs about: learning Python, getting starting in programming, how you can join the global Code in Place community, and more.

AMA link: [https://www.reddit.com/r/AMA/comments/1j87jux/im\_chris\_piech\_a\_stanford\_cs\_professor\_passionate/](https://www.reddit.com/r/AMA/comments/1j87jux/im_chris_piech_a_stanford_cs_professor_passionate/)

This is the perfect chance to get tips, insights, and guidance directly from someone who teaches programming, and is passionate about making coding more accessible.

Drop your questions or just come learn something new!",1741882902.0,computerscience,STEM
1jaz6pt,Concerning Debugging in TEA and the TEA Software Operating Environment,"**---[RESEARCH ENTRY]:**

**TITLE:** Concerning Debugging in TEA and the TEA Software Operating Environment

**AUTHOR:** Joseph W. Lutalo (jwl@nuchwezi.com, Nuchwezi ICT Research)

**KEYWORDS:** Software Engineering, Software Debugging, Debuggers, Text Processing Languages, TEA

**---[ABOUT]:**

Inspired by friends - Prof. M. Coblenz (UC San Diego) and his doctoral student, Hailey Li whose study on practical software debugging I got a chance to recently participate in, it came to my notice there was a need to fill a knowledge gap in how the important matter of debugging is catered for in the still young TEA programming language from my lab. The ideas in this paper though, definitely are of use to researchers and practitioners of software engineering and in particular software debugging in general.",1741940983.0,computerscience,STEM
1j9kfyz,CS research,"Hi guys, just had an open question for anyone working in research - what is it like? What do you do from day to day? What led you to doing research as opposed to going into the industry? 
I’m one of the run of the mill CS grads from a state school who never really considered research as an option, (definitely didn’t think I was smart enough at the time) but as I’ve been working in software development, and feeling, unfulfilled by what I’m doing- that the majority of my options for work consist of creating things or maintaining things that I don’t really care about, I was thinking that maybe I should try to transition to something in research.
Thanks for your time! Any perspective would be awesome.
",1741787179.0,computerscience,STEM
1ja1bws,On Many : One reductions and NP Completeness Proofs,"When I was in undergrad and studying computability and complexity, my professor started out the whole ""Does P = NP?"" discussion with basically the following:

> Let's say I know how get an answer for P. I don't know how to answer Q. But if I can translate P into Q in polynomial time, then I can get an answer for Q in polynomial time *if* I can get an answer for P in polynomial time.  

At least, that was my understanding at the time, and I'm paraphrasing because it's been a long time and I'm a little drunk.  

Also, I remember learning that if we can show that a language is NPC, and we can show that some NPC language is P-time computable, then we can show all NPC languages are P-time computable.  

In combination, this made me think that in order to show that some language is NPC, we need to find a many : one reduction from *that language* to *some NPC language*.  

This is, of course, backwards. Instead, we need to show that some NPC language is many : one reducible to a language we're trying to prove is NPC. But this never made intuitive sense to me and I always screwed it up. 

Part of the problem was what I learned in undergrad, the other part was that we used the Sipser text that was 90% symbols and 0% comprehensible English.

Until, nearly 20 years later, I was thumbing through my Cormen et al. *Introduction to Algorithms* book, and noticed that it has a section on NP completeness. It explained, in perfectly rational English, that the whole idea behind showing some language L is NP complete, is to show that some NPC language can be many : one reduced to that language, after showing L is in NP. And the rationale is that, if we know the difficulty of the NPC language, and can reduce it to L, then we know that L is *no harder than* the NPC language.  That is, if every instance of the NPC language can be solved using an instance of L, then we know that L is *no harder than* the NPC language.

My mind was blown. Rather than looking for ""how to solve L using an NPC language,"" we're looking to show, ""L is not harder than some NPC language.""

So all of this is to say, if you're struggling with NPC reductions and proofs and don't understand the ""direction"" of the proofs like I've been struggling with *for 20 years,* read the Cormen book's explanation on the proofs. I don't know how I missed this for years and years, but it finally made it all click for me after years and years. 

Hope this helps if you keep thinking of reductions backwards like I have for all these years.",1741831472.0,computerscience,STEM
1j8q0xv,How does CPU knows how to notify OS when a SysCall happen?,"Supposing P1 has an instruction that makes a Syscall to read from storage, for example. In reality, the OS manage this resource, but my doubt is, the program is already in memory and read to be executed by the CPU which will take that operation and send it to the storage controller to perform it, in this case, an i/o operation. Suppose the OS wants to deny the program from accessing the resource it wants, how the OS sits in between the program and CPU to block it if the program is already in CPU and ready to be executed?

https://preview.redd.it/l7uei0dj32oe1.png?width=964&format=png&auto=webp&s=6e7cd71c089bdc4f9b5dd756d22f7f47f3ef2c92

I don't know if I was clear in my questioning, please let me know and I will try to explain it better.

Also,if you did understand it, please be as deep as you can in the subject while answering, I will be very grateful.",1741697582.0,computerscience,STEM
1j8b06a,"How does an IDE work, and really any other program?","I am having trouble articulating this question because my minuscule knowledge of CS, but here goes. How exactly does an IDE work, let’s say that it’s a Java IDE, what language is the IDE created in? And what compiles the IDE software? I’m trying to learn computer science, but I don’t have any teachers, and I feel like I have somewhat of a crumbling foundation and a weak grasp on the whole concept, I want to understand how every little bit makes something tick, but I always end up drowning in confusion, so help would be much appreciated!",1741644739.0,computerscience,STEM
1j8ayg6,How does a “window” work?,"How exactly do “screens” go on top of one another on a computer screen, really think about that, how does the computer “remember” all of the pixels that were “under” the bottom window when you close it out, and redisplay them? I’m trying to learn computer science, but I don’t have any teachers, and I feel like I have somewhat of a crumbling foundation and a weak grasp on the whole concept, I want to understand how every little bit makes something tick, but I always end up drowning in confusion, so help would be much appreciated!",1741644617.0,computerscience,STEM
1j8kmdp,"I'd like to read up on the following topic: (if there is info on it?) When given an unrooted tree, pick a node as the root, what patterns/relationships can be observed in the new tree that is formed compared to picking other nodes as the root?","To elaborate, are there any cool mathematical ideas that are formed? Any real life applications to choosing different roots? Are there any theorems on this? Is this a well researched topic or just a dead end lame idea?  


Potential question: Given an unrooted tree with n vertices can you choose a root such that the height of the tree is h where h is any natural number > 0 and <= n? Is there a way to prove it's only possible for some h? I haven't played around with this problem yet.

I feel like there could be some sort of cool game or other weird ideas here. Visually the notion of choosing different roots reminds me of the different shapes you get if you lay a tissue flat on a table and pick it up at different points, so I wouldn't be surprised if there are some sort of topological ideas going on here",1741675081.0,computerscience,STEM
1j8c09n,Lambda Calculus,"I have taken an interest in lambda calculus recently, however I have ran into an issue. Each textbook or course use different notation, there is Church notation, there is also notation that uses higher order functions and words to describe the process, another notation that I have encountered was purely mathematical I believe, it looked like church notation, but twice as long. It is a pity that while this field of computer science is appealing to me, I struggle to grasp it because of my uncertainty pertaining to which notation I should use. I don't enjoy the use of higher order functions since I want to form a deep understanding of these subjects, however I am not planning on writing page long functions either. Any good resources and advice on which notation I should use is welcome. Also I apologise if my english is not coherent, it is not my first language, if I have made any mistakes that hinder your understanding of my question, feel free to correct me. Thank you in advance :)

TLDR: Confusion about notation in lambda calculus; Displeasement with using higher order functions; Looking for advice on notation type and relevant resources.",1741647274.0,computerscience,STEM
1j8bonm,"Zoltan's FLOPs – GPU mini-grant, 1st iteration",,1741646496.0,computerscience,STEM
1j7zsvc,Circuit Compiler,"Recently I wrote a small compiler

It job is to take in a truth table e.g:

A B | X
--
0 0 | 1

0 1 | 1

1 0 | 0

1 1 | 1 

And output a circuit in the form of a Boolean expression, e.g:

((~A)&(~B))|((~A)&(B))|((A)&(B))

I was hoping that some people here would have some feedback on it!

Also if anyone knows of any events here is the UK that have beginners into compilers then please send a DM!

Here is the code: https://github.com/alienflip/cttube, for anyone interested 🙂
",1741616659.0,computerscience,STEM
1j7dcam,How to learn gpu architecture?,"Hey guys
Currently I am learning about computer graphics and graphics api
To enhance my knowledge about how graphics api processes things(and on a level of curiosity as well)
I have decided to learn about the gpu architecture 
But the issue is I have no clue where to begin with
Also I dont know a lot of cpu architecture(If it's essential)
Where should I begin?
Any book of courses(prefered)",1741543056.0,computerscience,STEM
1j6xkmi,What is the differences between Computer Engineering(CE)and Computer Science?(CS),,1741487439.0,computerscience,STEM
